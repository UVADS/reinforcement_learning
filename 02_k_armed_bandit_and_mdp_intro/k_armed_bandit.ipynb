{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The k-armed Bandit Problem\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: August 21, 2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "Reinforcement Learning, RS Sutton & AG Barto, 2nd edition. Chapter 2\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Understand the k-armed bandit problem\n",
    "- Simulate a k-armed bandit problem with $\\epsilon$-greedy actions\n",
    "- Understand how using $\\epsilon$-greedy actions can improve the policy\n",
    "- Explain how different factors affect Upper-Confidence-Bound action selection\n",
    "\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- k-armed bandit problem\n",
    "- simulation to measure value function\n",
    "- $\\epsilon$-greedy actions can improve policy\n",
    "- Upper-Confidence-Bound action selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the k-armed Bandit Problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![k_armed_bandit](./k_armed_bandit.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL evaluates the actions taken rather than providing the correct actions to the agent.  \n",
    "This distinguished RL from other types of learning.  \n",
    "\n",
    "k-armed bandit problems are important and illustrate key ideas.  \n",
    "They include the class of problems using one state.  \n",
    "Contextual bandit problems allow for different states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The problem**\n",
    "\n",
    "Imagine a slot machine with *k* arms, possibly with different values.  \n",
    "At each step, you pull an arm (the action) and receive the reward.  \n",
    "Assume stationarity: the rewards don't change over time.  \n",
    "Objective: maximize the expected discounted total reward over a horizon.\n",
    "\n",
    "Examples:\n",
    "- selecting between different investment opportunities\n",
    "- a doctor selects between different treatment options\n",
    "- a family tries to discover the best Chinese restaurant in town"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value of an action**\n",
    "\n",
    "Each action results in a reward, and we can think about mean reward: the action's *value*\n",
    "\n",
    "Value of action *a* denoted $q_*(a) = E[R_t | A_t=a]$  \n",
    "\n",
    "This is expected reward given action *a* is selected\n",
    "\n",
    "This quantity is typically unknown and must be estimated empirically\n",
    "\n",
    "$Q_t(a)$ denotes the estimated value of action $a$ at time $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exploration vs Exploitation and Greedy Actions**\n",
    "\n",
    "Getting a good estimate means repeatedly trying the actions.\n",
    "\n",
    "For actions with large variance, more sampling is required to accurately measure $q_*(a)$\n",
    "\n",
    "One action will be best or there will be a tie. Selecting the action with highest $Q_t(a)$ is the greedy action.\n",
    "\n",
    "If we are too greedy, we might not discover the best action.\n",
    "\n",
    "Balancing exploration and exploition depends on value estimates, uncertainties, and number of remaining steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action-value Methods**\n",
    "\n",
    "Measure the action value of $a$ as the mean reward when $a$ is taken: \n",
    "\n",
    "$Q_t(a) = \\frac{\\sum_{i=1}^{t-1}R_i \\cdot \\unicode{x1D7D9}_{A_i=a}}{\\sum_{i=1}^{t-1} \\unicode{x1D7D9}_{A_i=a}}$\n",
    "\n",
    "By the law of large numbers, $Q_t(a)$ converges to $q_*(a)$ for sufficiently large *t*\n",
    "\n",
    "Simplest policy: select action with highest $Q_t$\n",
    "\n",
    "$A_t=\\underset{a}{\\operatorname{\\arg max}} Q_t(a)$\n",
    "\n",
    "This is the greedy action; it is generally better to balance with exploring, so $\\epsilon$-greedy methods may work better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Incremental Implementation**\n",
    "\n",
    "Want to produce a scalable way to compute $Q_t$ that doesn't require storing all rewards.  \n",
    "\n",
    "$\n",
    "\\begin{aligned}  \n",
    "Q_{n+1} &= \\frac{1}{n} \\sum_{i=1}^{n}R_i \\\\\n",
    "&= \\frac{1}{n} (R_n + \\sum_{i=1}^{n-1}R_i) \\\\\n",
    "&= Q_n + \\frac{1}{n}(R_n - Q_n) \\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "See Sutton & Barto for details\n",
    "\n",
    "Notice this involves storing two things (and not all the rewards): prior value estimate $Q_n$ and number of time steps $n$\n",
    "\n",
    "This is an important update pattern: \n",
    "\n",
    "**new estimate = old estimate + step_size * (new reward - old estimate)** \n",
    "\n",
    "The term (new reward - old estimate) is an error estimate\n",
    "\n",
    "If new reward = old estimate, then update is unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bandit Example**\n",
    "\n",
    "Below we set up a 4-armed bandit where each arm follows normal distribution with different mean, standard deviation.  \n",
    "Note the first arm has highest mean and also highest standard deviation.  \n",
    "The second arm might be mistaken for highest-valued arm as it has much lower standard deviation with slightly lower mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandit_one_draw():\n",
    "    arm = np.zeros(4)\n",
    "    arm[0] = np.random.normal(10,10)\n",
    "    arm[1] = np.random.normal(9.5,1)\n",
    "    arm[2] = np.random.normal(1,1)\n",
    "    arm[3] = np.random.normal(0.5,1)\n",
    "    return arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce one draw from each arm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_one_draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1a**  \n",
    "Write simulation code to implement k-armed bandit using the incremental update equation.   \n",
    "This should include:\n",
    "- Writing and calling a method to use $\\epsilon$-greedy actions  \n",
    "- Using a loop\n",
    "- Calling `bandit_one_draw()`  \n",
    "- Setting k=4, iterations = 100, $\\epsilon$=0.1\n",
    "- Inside the loop, print Q, the value estimates for each state. This will show the evolution through time.\n",
    "\n",
    "Does your simulation identify the first state as highest valued? What happens to your result if iterations = 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "k = 4\n",
    "iterations = 10000\n",
    "epsilon = 0.1\n",
    "\n",
    "Q = np.zeros(k)\n",
    "N = np.zeros(k)\n",
    "\n",
    "def act(epsilon, action_values):\n",
    "    action_size = len(action_values)\n",
    "    if np.random.rand() <= epsilon: # random draw with prob epsilon\n",
    "        return random.randrange(action_size)\n",
    "    return np.argmax(action_values)  # returns action\n",
    "\n",
    "results = np.empty((iterations, k))\n",
    "\n",
    "for it in range(iterations):\n",
    "    A = act(epsilon, Q)\n",
    "    draw = bandit_one_draw()\n",
    "    R = draw[A]\n",
    "    N[A] = N[A] + 1\n",
    "    Q[A] = Q[A] + (1/N[A]) * (R - Q[A])\n",
    "    print(Q)\n",
    "    results[it,:] = Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1b**  \n",
    "Modify your code from exercise 1a to store the Q values into an array.  \n",
    "The rows represent time steps, the columns represent actions.  \n",
    "Plot the Q values to show the evolution of the values over time for each action.  \n",
    "Include a legend to identify the actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(results)\n",
    "plt.legend(['0','1','2','3'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upper-Confidence-Bound Action Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We studied greedy policy\n",
    "\n",
    "$A_t=\\underset{a}{\\operatorname{\\arg max}} Q_t(a)$\n",
    "\n",
    "and the $\\epsilon$-greedy policy\n",
    "\n",
    "Can be useful to consider actions which may be promising (or avoid uncertain actions)\n",
    "\n",
    "*Upper-Confidence-Bound (UCB) action selection* can be useful here.\n",
    "\n",
    "The approach (below) accounts for variance in estimate of *a*, which appears under square root.\n",
    "\n",
    "Full term in parenthesis is an upper bound on true value of *a*\n",
    "\n",
    "Here, *c* is confidence level term. $N_t(a)$ is number of times *a* is selected. *t* is number of trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ucb](./ucb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2a**  \n",
    "What happens to the upper bound as the number of times *a* is selected increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "As $a$ is selected, $N_t(a)$ will increase and this will lower the standard deviation and the upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2b**  \n",
    "What happens to the upper bound as the number of times that other actions besides *a* is selected increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Since action $a$ is not selected, $t$ will increase but $N_t(a)$ will not. The standard deviation and the upper bound will increase. The increases will get smaller over time due to the $ln()$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
