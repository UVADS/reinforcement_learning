{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes (MDPs)\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: January 21, 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "Reinforcement Learning, RS Sutton & AG Barto, 2nd edition. Chapter 3\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Understand the properties of Markov Decision Processes\n",
    "- Describe the gains process and the agent's objective in RL\n",
    "- Explain optimal policy and value functions\n",
    "- Understand how the Bellman equation works \n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Markov Process\n",
    "- Markov Decision Process\n",
    "- Estimating the value of each action in each state\n",
    "- Estimating the value of each state given optimal action selection\n",
    "- optimal policy\n",
    "- optimal state value function, optimal action value function\n",
    "- dynamics function\n",
    "- discounting\n",
    "- gains process\n",
    "- Bellman equation\n",
    "- backup diagram\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing the Markov Decision Process (MDP) Formalism\n",
    "\n",
    "In reinforcement learning, time-varying state spaces are often modeled with Markov Decision Processes (MDP).  \n",
    "\n",
    "For each time step *t*, an agent observes the current state of the environment $S_t$,  \n",
    "takes action $A_t$, receives reward $R_{t+1}$, and transitions to new state $S_{t+1}$.\n",
    "\n",
    "MDPs originate from field of *optimal control*\n",
    "\n",
    "Very useful and applicable framework to decision-learning problems based on three signals:\n",
    "- choices made by agent (actions)\n",
    "- the basis on which choices are made (states)\n",
    "- signal to define agent's goal (rewards)\n",
    "\n",
    "Some important properties:\n",
    "\n",
    "- they can model sequential decision-making\n",
    "- the agent can be in a set of different states (finite or continuous)\n",
    "- actions can be discrete or continuous\n",
    "- the rewards are short-term, but collectively result in long-term value\n",
    "\n",
    "A word about the **Markov** property:\n",
    "\n",
    "This means that the future state will depend only on the current state. These sorts of processes are much easier to model.  \n",
    "\n",
    "A Markov decision process is a 4-tuple $(S,A,P,R)$\n",
    "\n",
    "$S$ is the state space  \n",
    "$A$ is the action space  \n",
    "$P$ is the probability transition matrix  \n",
    "$R$ is the reward\n",
    "\n",
    "Sometimes a discount factor $\\gamma$ is also included in the tuple.  \n",
    "Discounting future rewards is important for capturing time value of money, which is a finance concept.\n",
    "\n",
    "This diagram illustrates the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mdp](./mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trajectories**\n",
    "\n",
    "From the diagram, an agent's trajectory unfolds like this:\n",
    "\n",
    "$S_0$, $A_0$, $R_1$, $S_1$, $A_1$, $R_2$, $S_2$, $A_2$, $R_3$, ...   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Horizon**\n",
    "\n",
    "Sequential decision making problems may be *finite horizon* or *infinite horizon*.\n",
    "\n",
    "For a finite horizon *T*, nothing matters after this time ... things end.\n",
    "\n",
    "Going forward, we  will consider infinite horizon problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dynamics**\n",
    "\n",
    "For *finite MDP* (finite sets of states, actions, rewards), $R_t$ and $S_t$ have discrete probability distns.  \n",
    "Distributions only depend on preceding state and action.  \n",
    "Transition probability $p$ defines the dynamics:\n",
    "\n",
    "$p(s',r|s,a) = P[S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a]$\n",
    "\n",
    "Note: For many real-world problems, it is not possible to specify $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goals and Rewards\n",
    "\n",
    "Rewards are short-term in nature. **The agent will have this objective: maximize total expected discounted reward.**  \n",
    "It will attempt to learn the optimal policy (mapping states to actions) to reach this objective.  \n",
    "This is what it means to solve the reinforcement learning problem.\n",
    "\n",
    "To this end, we first define the *gains process* or *return* as a function of time.  \n",
    "This is the sum of future rewards discounted to time $t$:\n",
    "\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n",
    "\n",
    "Rewards may be random variables, so we work with expected gains.\n",
    "\n",
    "The agent attempts to maximize expected gains by discovering the best action from each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keeping the Gain Finite\n",
    "\n",
    "For infinite horizon problems, total reward can go to infinity.\n",
    "\n",
    "By discounting, the gain becomes finite:\n",
    "\n",
    "$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} <= \\sum_{k=0}^{\\infty} \\gamma^k R_{max} \n",
    "     = \\frac{R_{max} }{1 - \\gamma} $\n",
    "     \n",
    "where $R_{max}$ is the maximum reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policies and Value Functions\n",
    "\n",
    "Denote policy as $\\pi$\n",
    "\n",
    "A policy is a mapping from each state to probabilities of selecting each action\n",
    "\n",
    "It might be that each state maps to a single action with probability 1 (no uncertainty)\n",
    "\n",
    "The optimal policy is denoted $\\pi_*$\n",
    "\n",
    "**state-value functions**\n",
    "\n",
    "It is important to understand the value of starting from a state and following a given policy.\n",
    "\n",
    "This is denoted $v_{\\pi}(s)$\n",
    "\n",
    "Definition using the gains process:\n",
    "\n",
    "$v_\\pi(s) = \\mathbb{E}_{\\pi}[G_t | S_t=s] = \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1} | S_t=s \\right]$ for all $s \\in S$\n",
    "\n",
    "**action-value functions**\n",
    "\n",
    "The agent will want to improve upon a policy. To do this, it must measure the value of  \n",
    "\n",
    "1) starting in state $s$  \n",
    "2) trying action $a$  \n",
    "3) following policy $\\pi$ for the remainder of the actions\n",
    "\n",
    "This quantity, which is fundamental in Q-Learning, is denoted $q_{\\pi}(s, a)$\n",
    "\n",
    "Definition using the gains process:\n",
    "\n",
    "\\begin{align*}\n",
    "q_\\pi(s,a) & =  \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a] \\\\\n",
    "& =  \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1} | S_t=s, A_t=a \\right] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "for \\: all \\: s \\in S, a \\in A\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backup Diagram\n",
    "\n",
    "*Backup diagrams* are often used to explain the dynamics in reinforcement learning approaches.  \n",
    "Starting from state $s$ at the top, the agent has a policy which can select from three actions.  \n",
    "After an action is selected (shown as $a$ here), the environment responds with one of two possible states.  \n",
    "Transitioning to the state at far right, the agent collects immediate reward $r$ and moves to state s'.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![v_backup_diagram](./v_backup_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equation\n",
    "\n",
    "The Bellman equation computes the value of state $s$ by weighting each possible path by its probability. \n",
    "\n",
    "The value functions can be written in a recursive form. The *Bellman equation* for $v_{\\pi}$ is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "v_\\pi(s) &= \\mathbb{E}_{\\pi}[G_t | S_t=s] \\\\ \n",
    "         &= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t=s] \\\\ \n",
    "         &= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s]\\\\ \n",
    "         &= \\sum_{a}\\pi(a|s) \\sum_{s', r}p(s',r|s,a)  \\left[r + \\gamma v_{\\pi}(s') \\right]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantity $\\pi(a|s)$ allows for a stochastic policy where action $a$ is taken with some probability given state $s$.\n",
    "\n",
    "Notice what the equation says: the value of a state for a given policy is a function of immediate reward $r$ as well as the discounted value of the next state. This is the recursion.\n",
    "\n",
    "The expression is an expected value over all combinations of $r$, $s'$, and $a$.  \n",
    "The triple $(r,s',a)$ is weighted by probability $\\pi(a|s)p(s',r|s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent solves the reinforcement learning problem (and the Bellman equation) by finding the best policy $\\pi_*$\n",
    "\n",
    "This policy, which uses the best action from each state, will maximize reward in each state.\n",
    "\n",
    "The optimal state value function is defined as: \n",
    "\n",
    "$v_*(s) := \\underset{\\pi}{\\operatorname{\\max}} v_{\\pi}(s)$ for all states\n",
    "\n",
    "The optimal action value function is defined as: \n",
    "\n",
    "$q_*(s,a) := \\underset{\\pi}{\\operatorname{\\max}} q_{\\pi}(s,a)$\n",
    "\n",
    "Our efforts will be to learn these quantities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
