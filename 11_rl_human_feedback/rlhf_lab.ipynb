{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Ungraded Lab: Reinforcement Learning with Human Feedback (RLHF)\n",
        "\n",
        "Last updated: April 15, 2025\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gTeLuZd5q8qD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Source: ChatGPT created this lab from this prompt:\n",
        "\n",
        "*Create a lab assignment that demonstrates RLHF. Write and show the python code. Make sure the code works successfully.*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "StgrphcCrIBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Background\n",
        "\n",
        "RLHF is used to train models (like ChatGPT) by aligning behavior with human preferences rather than just predefined rewards.\n",
        "\n",
        "We'll simulate RLHF in a basic environment where an agent must learn to generate the most preferred string from a predefined set.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "opmRKKX3rd6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code"
      ],
      "metadata": {
        "id": "ojKnmtaLsgqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Packages"
      ],
      "metadata": {
        "id": "iCDX7D3duIt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "mycjOYlDuHZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Configs"
      ],
      "metadata": {
        "id": "KAtbdomNuM8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a fixed vocabulary\n",
        "VOCAB = [\"A\", \"B\", \"C\"]\n",
        "TARGET_STRING = [\"A\", \"B\", \"C\"]  # This is the \"ideal\" sequence from a human preference view\n",
        "\n",
        "epochs = 10\n",
        "n_pairs = 10"
      ],
      "metadata": {
        "id": "pJcyddYpuTux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculate preferences"
      ],
      "metadata": {
        "id": "cJaKgzdGuPdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_sequence(length=3):\n",
        "    return np.random.choice(VOCAB, size=length).tolist()\n",
        "\n",
        "# Preference function simulating human feedback\n",
        "def human_preference(seq1, seq2):\n",
        "    # Compare how close each sequence is to the target\n",
        "    score1 = sum([1 for a, b in zip(seq1, TARGET_STRING) if a == b])\n",
        "    score2 = sum([1 for a, b in zip(seq2, TARGET_STRING) if a == b])\n",
        "    if score1 > score2:\n",
        "        return 0  # Prefer seq1\n",
        "    else:\n",
        "        return 1  # Prefer seq2\n"
      ],
      "metadata": {
        "id": "Hz6mngl6ronE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simulate preferences from human over random pairs"
      ],
      "metadata": {
        "id": "pkqqk37ot9tV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_preferences(n_pairs):\n",
        "    preferences = []\n",
        "    for _ in range(n_pairs):\n",
        "        s1 = generate_random_sequence()\n",
        "        s2 = generate_random_sequence()\n",
        "        pref = human_preference(s1, s2)\n",
        "        preferences.append((s1, s2, pref))\n",
        "    return preferences\n",
        "\n",
        "preferences = collect_preferences(n_pairs)\n",
        "print(\"Sample preference:\", preferences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT58KM86s08f",
        "outputId": "1c00015b-3dda-424f-a4dd-b71320c284a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample preference: (['A', 'A', 'C'], ['A', 'A', 'C'], 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train a Reward Model from Preferences\n"
      ],
      "metadata": {
        "id": "Tlv_yHHZt4wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reward model: assign a reward value to each token at each position\n",
        "reward_table = defaultdict(lambda: np.random.randn())\n",
        "\n",
        "def sequence_reward(seq):\n",
        "    return sum(reward_table[(i, token)] for i, token in enumerate(seq))\n",
        "\n",
        "# Train the reward model using preference data\n",
        "def train_reward_model(preferences, epochs=5, lr=0.01):\n",
        "    for epoch in range(epochs):\n",
        "        for s1, s2, pref in preferences:\n",
        "            r1 = sequence_reward(s1)\n",
        "            r2 = sequence_reward(s2)\n",
        "\n",
        "            # Apply logistic loss\n",
        "            prob1 = np.exp(r1) / (np.exp(r1) + np.exp(r2))\n",
        "            grad = (1 - prob1) if pref == 0 else -prob1\n",
        "\n",
        "            # explanation:\n",
        "            # if pref = 0 (human prefers s1)\n",
        "            # but prob1 = 0 (extreme case), we incur max loss. grad will push prob1 closer to 1\n",
        "\n",
        "            # Update reward table\n",
        "            for i, token in enumerate(s1):\n",
        "                reward_table[(i, token)] += lr * grad\n",
        "            for i, token in enumerate(s2):\n",
        "                reward_table[(i, token)] -= lr * grad\n",
        "\n",
        "            # explanation\n",
        "            # if grad > 0, we increase the reward for s1 and decrease for s2\n",
        "            # if grad < 0, we decrease the reward for s1 and increase for s2\n",
        "            # example: if s1 has 'A' in first position and s2 has 'B',\n",
        "            # there will be a preference at this position for s1\n",
        "            # the reward model should learn this preference and give high prob1\n",
        "            # we compute grad of prob1 and nudge reward table entries\n",
        "            # value of reward_table[0,'A'] goes up\n",
        "            # value of reward_table[0,'B'] goes down\n",
        "\n",
        "train_reward_model(preferences)"
      ],
      "metadata": {
        "id": "JPL7_ZFl6-Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Learn preferred sequence"
      ],
      "metadata": {
        "id": "LO1CHcHG3ITA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_sample():\n",
        "    # Sample best token for each position according to reward_table\n",
        "    best_seq = []\n",
        "    for i in range(3):\n",
        "        best_token = max(VOCAB, key=lambda token: reward_table[(i, token)])\n",
        "        best_seq.append(best_token)\n",
        "    return best_seq\n",
        "\n",
        "final_sequence = policy_sample()\n",
        "print(\"Learned preferred sequence:\", final_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQn-Ds0bu8uK",
        "outputId": "d7a544e9-c866-4636-a9e2-d1b323f3212e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned preferred sequence: ['C', 'C', 'A']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### Tasks\n",
        "\n",
        "1) Test that `human_preference() works properly`"
      ],
      "metadata": {
        "id": "-_26DRdRsOga"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BsB75q5Zr0Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Call the function `collect_preferences()` with parameter `n_pairs=10`  \n",
        "Does the output make sense?"
      ],
      "metadata": {
        "id": "7cd9I5RMtMPY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SIuE6jERr351"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Let's see if the \"humans\" can teach the model how to learn.  \n",
        "Call `train_reward_model()`, modifying `n_pairs`, `epochs` as needed so the learned sequence matches TARGET_STRING = [\"A\", \"B\", \"C\"]"
      ],
      "metadata": {
        "id": "iWzMQRAN3ReY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFUVswfCtGqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Review `train_reward_model()` to understand what it's doing"
      ],
      "metadata": {
        "id": "drJFRcdC9gFY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tmoQDCNv9jz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5xBFDSaI1w7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}