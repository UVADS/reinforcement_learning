{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradients\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "### Last updated: March 16, 2025\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "- Reinforcement Learning, RS Sutton & AG Barto, 2nd edition. Chapter 13\n",
    "- Mastering Reinforcement Learning with Python. E Bilgin. Chapter 7: Policy-Based Methods.\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Understand policy gradient methods\n",
    "- Understand a policy gradient algorithm: REINFORCE\n",
    "- Explain the limitation of REINFORCE as a practical method\n",
    "- Explain the benefit of using cost-to-go\n",
    "- Understand extensions of REINFORCE: why is a baseline useful\n",
    "- Understand why a critic is useful\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- policy gradient methods\n",
    "- parametrizing the policy\n",
    "- estimating the gradient for REINFORCE\n",
    "- cost-to-go\n",
    "- baseline\n",
    "- actor-critic methods\n",
    "\n",
    "### ORDERING\n",
    "\n",
    "To follow Deep Q-Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Introducing Policy-Based Methods\n",
    "\n",
    "Most of our work has been with value functions (e.g., action-value estimates). \n",
    "\n",
    "This is an indirect yet powerful way to tackle sequential prediction and control problems\n",
    "\n",
    "Now we do something different: **directly model the policy and work to improve it**\n",
    "\n",
    "What does this buy us?\n",
    "\n",
    "- arguably more principled since policy methods directly optimize the policy parameters  \n",
    "  this is more efficient in some cases\n",
    "\n",
    "- allows us to model continuous action spaces\n",
    "\n",
    "- can learn truly random stochastic policies  \n",
    "  in some cases the optimal policy is a mix of two actions with fixed probabilities (bluff or not in Poker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Implementation of Policy-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy is parametrized**\n",
    "\n",
    "Denote the *policy parameter* $\\boldsymbol{\\theta}$ which is a vector\n",
    "\n",
    "The policy is parametrized as: \n",
    "\n",
    "$\\pi(a|s,\\boldsymbol{\\theta}) = P(A_t=a | S_t=s, \\boldsymbol{\\theta_t}=\\boldsymbol{\\theta})$\n",
    "\n",
    "The parametrization is flexible, but $\\pi(a|s,\\boldsymbol{\\theta})$ must be differentiable wrt $\\boldsymbol{\\theta}$\n",
    "\n",
    "Will define an objective function which uses an expectation, which will be differentiable\n",
    "\n",
    "The goal is to learn how to update the parameter components to improve performance measure $J$\n",
    "\n",
    "**Objective function**\n",
    "\n",
    "The objective will be the long-term value measured from a state $s_0$\n",
    "\n",
    "$ J(\\boldsymbol{\\theta_t}) = v_{\\pi _{\\boldsymbol{\\theta_t}}} (s_0) $\n",
    "\n",
    "We can write in terms of expectation:\n",
    "\n",
    "$ J(\\boldsymbol{\\theta_t}) = \\mathbb{E}_{\\pi} \\left[\\sum_{t=0}^T \\gamma^t R_t | S_t=s_0 \\right] $\n",
    "\n",
    "The discounted reward expression is sometimes abbreviated compactly:\n",
    "\n",
    "$ J(\\boldsymbol{\\theta_t}) = \\mathbb{E}_{\\pi} [r(\\tau)] $\n",
    "\n",
    "To improve the policy, we use the gradient of $ J(\\boldsymbol{\\theta_t}) $ wrt policy parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### III. Policy Gradient Theorem\n",
    "\n",
    "How can we change policy parameter to ensure improvement?\n",
    "\n",
    "*Policy gradient theorem* provides analytic expression for gradient wrt policy parameters\n",
    "\n",
    "For the episodic case, it begins with this fact:\n",
    "\n",
    "Gradient of state-value function can be written in terms of action-value function:\n",
    "\n",
    "$ \\nabla v_{\\pi}(s) = \\nabla \\left[ \\sum_a \\pi(a|s) q_{\\pi}(s,a) \\right] $\n",
    "\n",
    "and results in formula\n",
    "\n",
    "$ \\nabla J(\\boldsymbol{\\theta_t}) \\propto \\sum_s \\mu (s) \\sum_a q_{\\pi}(s,a) \\nabla\\pi(a|s,\\boldsymbol{\\theta_t}) $\n",
    "\n",
    "where $\\mu $ is on-policy distribution under $\\pi$\n",
    "\n",
    "Details in Sutton & Barto p325\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte Carlo Sampling and $\\mu$**\n",
    "\n",
    "When we face expectations involving distributions like $\\mu$,\n",
    "\n",
    "One approach is to sample a large number of trajectories and average (Monte Carlo)\n",
    "\n",
    "For large enough samples, the result will converge to true value\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Gradient Ascent to make improvements**\n",
    "\n",
    "We will follow the direction of the gradient to improve $\\boldsymbol{\\theta}$\n",
    "\n",
    "The update equation has form:\n",
    "\n",
    "$\\boldsymbol{\\theta_{t+1}} = \\boldsymbol{\\theta_t} + \\alpha \\hat{\\nabla  J(\\boldsymbol{\\theta_t})} $\n",
    "\n",
    "where \n",
    "\n",
    "- $\\alpha$ is the step size (learning rate)\n",
    "\n",
    "- $\\hat{\\nabla  J(\\boldsymbol{\\theta_t})}$ is a stochastic estimate of the gradient of the performance measure with respect to the parameters. \n",
    "\n",
    "On average, $\\hat{\\nabla  J(\\boldsymbol{\\theta_t})}$ should approach the true gradient.\n",
    "\n",
    "It is common to use a softmax distribution for the policy:\n",
    "\n",
    "$\\pi(a|s,\\boldsymbol{\\theta}) = \\frac{\\exp{h(s,a,\\boldsymbol{\\theta})}}{\\sum_b \\exp{h(s,a,\\boldsymbol{\\theta})}}$\n",
    "\n",
    "where $h(s,a,\\boldsymbol{\\theta})$ is a parametrized numerical preference given state $s$ and action $a$.\n",
    "\n",
    "higher preference leads to higher probability\n",
    "\n",
    "This is where modeling comes in, as $h(s,a,\\boldsymbol{\\theta})$ could be any sort of model (linear, deep neural network, ...).\n",
    "\n",
    "Methods following this approach are called are policy gradient (PG) methods.\n",
    "\n",
    "Methods that learn approximations to policy and value functions are called *actor-critic methods*.  \n",
    "*Actor*: the learned policy  \n",
    "*Critic*: the learned value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### IV. Our First Policy-Gradient Algorithm: REINFORCE\n",
    "\n",
    "REINFORCE is a Monte Carlo Policy Gradient algorithm\n",
    "\n",
    "Need to sample such that the expectation of the sample gradient is proportional to population gradient. \n",
    "\n",
    "The policy gradient theorem (see Sutton & Barto Ch 13 for details) gives an expression to obtain unbiased samples.\n",
    "\n",
    "From earlier,\n",
    "\n",
    "$ \\nabla J(\\boldsymbol{\\theta_t}) \\propto \\sum_s \\mu (s) \\sum_a q_{\\pi}(s,a) \\nabla\\pi(a|s,\\boldsymbol{\\theta_t}) $\n",
    "\n",
    "$ \\nabla J(\\boldsymbol{\\theta_t}) = \\mathbb{E}_{\\pi} \\left[ \\sum_a q_{\\pi}(S_t,a) \\nabla\\pi(a|S_t,\\boldsymbol{\\theta_t}) \\right] $\n",
    "\n",
    "This involves sum over states $S_t$ weighted by how often states occur under target policy $\\pi$\n",
    "\n",
    "Next, we introduce $A_t$ in a similar way. We want to weight each term by probability of action $a$.\n",
    "\n",
    "Trick: multiply by $\\pi$ and then divide to maintain equality:\n",
    "\n",
    "$ \\nabla J(\\boldsymbol{\\theta_t}) = \\mathbb{E}_{\\pi} \\left[ \\pi(a|S_t,\\boldsymbol{\\theta_t}) q_{\\pi}(S_t,a)  \\frac{\\nabla\\pi(a|S_t,\\boldsymbol{\\theta_t})}{\\pi(a|S_t,\\boldsymbol{\\theta_t})} \\right] $\n",
    "\n",
    "$ \\nabla J(\\boldsymbol{\\theta_t}) = \\mathbb{E}_{\\pi} \\left[ q_{\\pi}(S_t,A_t)  \\frac{\\nabla\\pi(A_t|S_t,\\boldsymbol{\\theta_t})}{\\pi(A_t|S_t,\\boldsymbol{\\theta_t})} \\right] $\n",
    "\n",
    "This replaces $a$ by the sample $A_t \\sim \\pi $\n",
    "\n",
    "Finally, we substitute the gain $G_t$ (return of the trajectory):\n",
    "\n",
    "$ \\nabla J(\\boldsymbol{\\theta_t}) = \\mathbb{E}_{\\pi} \\left[ G_t \\frac{\\nabla\\pi(A_t|S_t,\\boldsymbol{\\theta_t})}{\\pi(A_t|S_t,\\boldsymbol{\\theta_t})} \\right] $\n",
    "\n",
    "\n",
    "The update equation (excluding discounting), which uses stochastic gradient ascent, has form:\n",
    "\n",
    "$\\boldsymbol{\\theta_{t+1}} = \\boldsymbol{\\theta_t} + \\alpha G_t \\frac{\\nabla \\pi(A_t|S_t, \\boldsymbol{\\theta_t})}{\\pi(A_t|S_t, \\boldsymbol{\\theta_t})} $\n",
    "\n",
    "Since each trajectory must be fully simulated to compute $G_t$, this is a Monte Carlo method.\n",
    "\n",
    "**The intuition behind the equation:**\n",
    "\n",
    "- the parameter component update is proportional to the product of two factors:  \n",
    "  1) the return or gain   \n",
    "  2) the gradient of the probability of taking the action divided by the probability of taking the action\n",
    "\n",
    "- $G_t$ in the numerator will act to directly increase the update in that direction\n",
    "- the probability of action $A_t$ in the denominator will act to decrease the update in that direction\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation Callout**\n",
    "\n",
    "We will see expression\n",
    "\n",
    "$\\frac{\\nabla \\pi(A_t|S_t, \\boldsymbol{\\theta_t})}{\\pi(A_t|S_t, \\boldsymbol{\\theta_t})}$\n",
    "\n",
    "sometimes replaced with $\\nabla \\; ln \\; \\pi(A_t|S_t, \\boldsymbol{\\theta_t}) $\n",
    "\n",
    "This comes from the equality for the gradient of the $ln()$ function:\n",
    "\n",
    "$\\nabla ln(x) = \\frac{\\nabla x}{x}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REINFORCE Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs: \n",
    "- differentiable policy $\\pi(a|s,\\boldsymbol{\\theta})$\n",
    "- step size $\\alpha$\n",
    "- discount factor $\\gamma$\n",
    "- policy parameter $\\boldsymbol{\\theta_t}$\n",
    "\n",
    "Initialize $\\boldsymbol{\\theta_t}$\n",
    "\n",
    "Loop for each episode:  \n",
    "- Generate an episode $S_0,A_0,R_1,...,S_{T-1},A_{T-1},R_T$  \n",
    "- Loop for each time step $t=0,1,...,T-1$:\n",
    "\n",
    "    - $ G = \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_k$  (compute the discounted return of the time step)\n",
    "    \n",
    "    - $ \\boldsymbol{\\theta} = \\boldsymbol{\\theta} + \\alpha \\gamma^t G \\frac{\\nabla \\pi(A_t|S_t, \\boldsymbol{\\theta})}{\\pi(A_t|S_t, \\boldsymbol{\\theta})} $ (update the parameter vector using stochastic gradient ascent)\n",
    "\n",
    "where the parameter update equation uses $\\gamma^t$ for discounting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE Limitations\n",
    "\n",
    "REINFORCE has good theoretical convergence properties.\n",
    "\n",
    "By design, expected update in same direction as performance gradient.\n",
    "\n",
    "However, REINFORCE can have high variance and converge slowly.\n",
    "\n",
    "There are common tricks that can help including:\n",
    "\n",
    "- Replacing reward sum with *reward-to-go*\n",
    "\n",
    "- Using a baseline\n",
    "\n",
    "We look at these next\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward-to-go\n",
    "\n",
    "To improve the policy, we use an update step involving quantity $\\hat{\\nabla  J(\\boldsymbol{\\theta_t})}$\n",
    "\n",
    "This estimate calculates $G_t$ which is the return from time $t=0$\n",
    "\n",
    "Since we are at current time $t$, we cannot influence past rewards where $t'<t$\n",
    "\n",
    "Instead, we can consider only the rewards from current time $t'\\ge t$\n",
    "\n",
    "This sum $\\sum_{t'=t}^T R(s,a)$ is the *reward-to-go*.\n",
    "\n",
    "The idea is appealing since:\n",
    "- only rewards from current time forward can be influenced by the policy\n",
    "- excluding past rewards can reduce the variance in the estimate\n",
    "- the gradient estimate will still be unbiased\n",
    "\n",
    "In fact, the reward-to-go is an estimate for $Q(s,a)$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REINFORCE with Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE can be generalized to include a comparison of the action value to a baseline $b(s)$\n",
    "\n",
    "The baseline can reduce variance and speed up learning, without introducing bias\n",
    "\n",
    "Here is the update rule (excludes discounting):\n",
    "\n",
    "$\\boldsymbol{\\theta_{t+1}} = \\boldsymbol{\\theta_t} + \\alpha (G_t - b(s)) \\frac{\\nabla \\pi(A_t|S_t, \\boldsymbol{\\theta_t})}{\\pi(A_t|S_t, \\boldsymbol{\\theta_t})} $\n",
    "\n",
    "A useful baseline is an estimate of the state value $\\hat{v}(S_t, \\textbf{w})$\n",
    "\n",
    "The algorithm is very similar to REINFORCE but with these differences:\n",
    "- introduce baseline function as state-value function\n",
    "- step size for policy update and value function update\n",
    "- compute baseline\n",
    "- update equation for weights in state-value function\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![reinforce](./reinforce_w_baseline0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Step Actor-Critic\n",
    "\n",
    "**NOTE:** We introduce this class of models here and go deeper in the next module\n",
    "\n",
    "REINFORCE with baseline uses state-value function estimates based only on first state of each transition\n",
    "\n",
    "In actor-critic methods, the state-value function is also applied to the second state transition.  \n",
    "This allows for computing a one-step return, which can assess the action (acts as *critic*).\n",
    "\n",
    "Overall policy-gradient method called *actor-critic* method.\n",
    "\n",
    "This one-step method replaces full return of REINFORCE with one-step return. It is fully online and incremental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ac_one_step](./actor_critic_one_step0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Going Deeper\n",
    "\n",
    "We will dive further into Actor-Critic and other extensions next.  \n",
    "\n",
    "If you are interested in policy gradients applied to precision medicine, [here](https://arxiv.org/abs/1802.10440) is one example:\n",
    "\n",
    "*Precision medicine as a control problem: Using simulation and deep reinforcement learning to discover adaptive, personalized multi-cytokine therapy for sepsis.* \n",
    "\n",
    "Brenden K. Petersen, Jiachen Yang, Will S. Grathwohl, Chase Cockrell, Claudio Santiago, Gary An, Daniel M. Faissol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
