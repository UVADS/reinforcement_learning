{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches to Solving MDPs I: Dynamic Programming\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: January 24, 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOURCES \n",
    "\n",
    "- Reinforcement Learning, RS Sutton & AG Barto, 2nd edition. Chapter 4\n",
    "- Mastering Reinforcement Learning with Python, Enes Bilgin. Chapter 5\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Understand the conceptual ideas behind dynamic programming \n",
    "- Study the Bellman optimality equations for estimating the optimal value functions\n",
    "- Discuss the strengths and limitations of Dynamic Programming\n",
    "- Understand and apply the value iteration algorithm\n",
    "- Explain how generalized policy iteration works to find optimality\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Dynamic Programming\n",
    "- Value Iteration\n",
    "- Policy Iteration\n",
    "- Generalized Policy Iteration\n",
    "- Bellman optimality equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are three important approaches to solving MDPs:\n",
    "\n",
    "- Dynamic Programming (DP)\n",
    "- Monte Carlo Simulation (MC)\n",
    "- Temporal Difference (TD)\n",
    "\n",
    "**DP methods**  \n",
    "Provide optimal solutions to Markov decision processes (MDPs)  \n",
    "Much more efficient than exhaustive search  \n",
    "Developed by Richard Bellman in the 1950s  \n",
    "\n",
    "Strengths: \n",
    "\n",
    "- Closed-form equations for optimality\n",
    "- Intuitive, as it exploits the property of overlapping substructure\n",
    "- Efficient computation\n",
    "\n",
    "Limitations:\n",
    "\n",
    "- Requires complete knowledge of the state transition and reward dynamics of the environment. This is often unrealistic.\n",
    "- When the state space is continuous or massive, the required runtime becomes infeasible\n",
    "\n",
    "**MC methods**  \n",
    "Model-free method that uses sampled transitions from the environment, so transition matrix (dynamics of system) is not required\n",
    "\n",
    "An update is made after a full trajectory is simulated\n",
    "\n",
    "Relies on convergence properties when measuring value functions\n",
    "\n",
    "**TD methods**  \n",
    "Model-free method that learns by bootstrapping from the current estimate of the value function. \n",
    "\n",
    "Samples from the environment like Monte Carlo, and performs updates based on current estimates, like dynamic programming.\n",
    "\n",
    "An update is made after each time step is simulated (learns faster than MC)\n",
    "\n",
    "In this notebook we will concentrate on Dynamic Programming.\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivating the Dynamic Programming Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only briefly touch on DP: it is very instructive but generally impractical for large-scale problems.\n",
    "\n",
    "Consider this example where a robot needs to decide the best path to traverse on a binary tree. \n",
    "\n",
    "At each time step $t$, the robot can move UP or DOWN (actions).\n",
    "\n",
    "This tree has three layers (ignoring the start, or root, node).\n",
    "Rewards are located on each node.\n",
    "\n",
    "We make this problem a little easier with these features:\n",
    "- discount factor will be $\\gamma=1$ (no discounting)\n",
    "- the robot decides which path to take (the action) with probability 1; deterministic policy $\\pi(s)$\n",
    "- the environment is deterministic: the robot will move in the intended direction\n",
    "\n",
    "Task: select the path that maximizes expected return. What is the path and return?  \n",
    "\n",
    "Is this problem Markov?\n",
    "\n",
    "**Example: Robot maximization task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_task1](./tree_task1_binary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working from left to right is a brute force method that solves the problem. \n",
    "\n",
    "Brute force -> redundant calculations. Is there a more efficient way?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Think about working backwards from right to left, backing the rewards to earlier layers in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[enter your ideas here] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting at the next-to-last layer, it will look ahead from each node to the future rewards.\n",
    "\n",
    "It will select the next step with highest reward, acting greedily.  It can compute this maximum before taking a step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_task1](./tree_task1_working_backwards_binary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example at the top node with reward 2, it would select the path leading to 3.  \n",
    "The value of the top node with reward 2 is then 2 + 3 = 5. It would receive immediate reward 2 and would receive 3 at the next step.\n",
    "\n",
    "Let's back up the rewards earlier in the tree and update the values at layer 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_task1](./tree_task1_working_backwards_adding_rewards1_binary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the structure: use the potential values in the next step $t+1$ to compute the value at the earlier step $t$\n",
    "\n",
    "This is iterative and can be easily coded for massive trees\n",
    "\n",
    "In general, **problems that have overlapping substructure are good candidates for dynamic programming solutions**\n",
    "\n",
    "Repeat this procedure, working backwards through the tree and pushing values earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_task1](./tree_task1_working_backwards_adding_rewards2_binary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree_task1](./tree_task1_working_backwards2_adding_rewards2_binary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces the best path (in green) and maximum reward of 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "A short exercise is found in *exercises_robot.ipynb* \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relationship to Pricing an American Call Option**  \n",
    "\n",
    "If you have priced options on a binomal lattice, you will notice similar ideas being applied.  \n",
    "\n",
    "A *call option* on a stock allows the holder to buy the stock (*exercise* the option) for a fixed price (*strike price*) by a fixed date (*expiry*).    \n",
    "An *American* call option allows the holder to exercise before expiry.  He will take the action - *(exercise, no exercise)* - that maximizes value.  \n",
    "\n",
    "For the option pricing problem, it is a bit more involved than the robot problem:\n",
    "\n",
    "- there will be discounting to reflect time value of money\n",
    "- the environment is stochastic: the stock price will fluctuate with a random component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Formulas in Dynamic Programming\n",
    "\n",
    "The formulas will handle the general case to include discounting, a policy that follows a distribution, and uncertainty in the environment.\n",
    "\n",
    "Here are the Bellman optimality equations for computing optimal state value $v_*(s)$ and optimal action value $q_*(s,a)$\n",
    "\n",
    "$v_*(s) = \\underset{a}{\\operatorname{\\max}} \\mathbb{E} [R_{t+1} + \\gamma v_*(S_{t+1}) | S_t=s]$\n",
    "\n",
    "\n",
    "$q_*(s,a) = \\mathbb{E} [R_{t+1} + \\gamma \\underset{a'}{\\operatorname{\\max}} q_*(S_{t+1},a') | S_t=s, A_t=a]$\n",
    "\n",
    "For $v_*(s)$, the agent selects actions that maximize the state value for each state *s*.\n",
    "\n",
    "For $q_*(s,a)$, the agent selects action *a* from state *s* and follows the optimal policy afterwards. \n",
    "\n",
    "In order to compute expectations, the transition probabilities need to be known in advance.\n",
    "\n",
    "Additionally, the expectation calculation grows with the number of states.\n",
    "\n",
    "Notice the structure: the Bellman equations are recursive. They can be easily programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Evaluation and Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important goal is to discover the optimal policy.  \n",
    "This can be done by measuring the value of the current policy, trying to improve the policy, and repeating these two steps.\n",
    "\n",
    "When environment dynamics are known, the Bellman equation can be solved iteratively.  \n",
    "The initial value estimates $v_0$ are chosen arbitrarily; in the terminal state, values are zero.\n",
    "\n",
    "The Bellman equation is turned into an update step.\n",
    "\n",
    "The *policy iteration* algorithm can be found in Sutton & Barto, but it is computationally expensive.  \n",
    "A shortcut method is *value iteration*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "The idea of value iteration is to refine estimates of the value function through the Bellman equation.  \n",
    "We start with random $V(s)$, we update with the action that maximizes the expected value, and we repeat until a criterion is met.\n",
    "\n",
    "Once we have estimates for $V(s)$ for all $s$, the policy will select actions that maximize expected value.  \n",
    "It can be greedy since all values reflect expected cumulative gains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![value_iteration](./value_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Policy Iteration (GPI)\n",
    "\n",
    "**Policy iteration** consists of two simultaneous, interacting processes: \n",
    "- *policy evaluation*: make the value function consistent with the current policy \n",
    "- *policy improvement*: make the policy greedy given the current value function\n",
    "\n",
    "The graphic below demonstrates this idea, using iteration to build a better policy and then value the updated policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gpi](./gpi1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly all RL problems are described by GPI.\n",
    "\n",
    "The two steps work in opposition to find the optimal value function and optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Since we often won't have the transition probabilities to solve real-world problems, we will need other methods.\n",
    "\n",
    "Next, we will study a technique that sidesteps the need for transition probabilities: Monte Carlo simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
