{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Machine Teaching\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: April 11, 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "- Mastering Reinforcement Learning with Python, Enes Bilgin. Chapter 10\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Explain the benefit of Machine Teaching (MT)\n",
    "- Describe different methods for MT\n",
    "- Apply Reward Shaping to teach an agent to reach a goal\n",
    "- Describe a method for preventing impossible / unwanted actions from a given state\n",
    "- Explain the idea of Curriculum Learning\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Machine Teaching\n",
    "- Concept (part of the skillset)\n",
    "- Reward Shaping\n",
    "- Action Masking\n",
    "- Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Machine Teaching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL requires a large amount of data since the agent may be only exposed to training examples...and not guidance\n",
    "\n",
    "*Machine Teaching (MT)* focuses on extracting knowledge from a *teacher*\n",
    "\n",
    "A teacher is a subject-matter expert on a topic\n",
    "\n",
    "This can be more efficient (fewer samples / less training / less compute needed)\n",
    "\n",
    "This is certainly true for humans learning from a teacher as well\n",
    "\n",
    "The teacher infuses knowledge into the machine\n",
    "\n",
    "But how to do this?\n",
    "\n",
    "We will touch on several approaches\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *concept* is a part of the skillset needed to solve a problem\n",
    "\n",
    "It can be helpful to break problems down this way, as it makes things easier\n",
    "\n",
    "Consider learning:\n",
    "\n",
    "- How to play chess if rewards are only given at the end.  \n",
    "  Can be helpful to have **intermediate rewards** (capturing a queen, having a strong opening)\n",
    "\n",
    "- How to play basketball if rewards are based on final score. Can be helpful for learning skills such as:\n",
    "  - Passing\n",
    "  - Rebounding\n",
    "  - Shooting\n",
    "  - Dribbling\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Reward Shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several properties of a problem can make it hard for the agent to learn the optimal policy, such as:\n",
    "\n",
    "- **Sparse rewards.** If feedback doesn't come often, it can be hard for the agent to learn if it's taking the right actions\n",
    "\n",
    "- **Attribution Problem.** When sparse rewards come, it is hard to know the action leading to the reward\n",
    "\n",
    "- **Qualitative objectives.** A task such as *walking* can be hard to define\n",
    "\n",
    "- **Multi-objective task.** It may be necessary for the agent to learn to balance multiple priorities.  \n",
    "    Autonomous driving involves learning how to balance things like:\n",
    "    - speed\n",
    "    - safety\n",
    "    - fuel efficiency\n",
    "  \n",
    "    Ultimately, these components need to be weighted and combined into a scalar value.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reward shaping* involves designing a function that moves the agent towards success states and away from failure states. \n",
    "\n",
    "Positive rewards are given for moving toward good states  \n",
    "Negative rewards are given for moving toward bad states \n",
    "\n",
    "There are important considerations for this to work well, such as:\n",
    "- Providing the right incentive to reach a goal (and not linger near the goal)\n",
    "- Providing the right incentive to end ASAP when needed\n",
    "- Taking into account the relative size of rewards\n",
    "\n",
    "**Warning:**  \n",
    "The agent learns behavior based on the rewards.  \n",
    "Sometimes the agent learns behavior that maximizes reward but isn't what the designer had in mind. \n",
    "\n",
    "**Question:** Suppose we design a reward function that gives a constant reward for all states. Will the agent learn? Explain your answer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at reward shaping examples.\n",
    "\n",
    "**Example 1: Shaped Reward Function**\n",
    "\n",
    "Imagine a robot that can have position in $[-1,1]$  \n",
    "The figure below shows a reward function for moving an agent toward the goal state=1  \n",
    "As the agent moves from 0 to 1, the reward grows larger (by the square of the state value)  \n",
    "As the agent moves from 0 to -1, the penalty grows larger \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./shaped_reward1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2: Designing Rewards to Prevent Sepsis**\n",
    "\n",
    "In the paper *Deep Reinforcement Learning for Sepsis Treatment* by Raghu et. al., the task is to prevent sepsis by controlling **SOFA** and **Lactate**. \n",
    "\n",
    "These measures are proxies for overall patient health.\n",
    "\n",
    "This is a multi-objective task.\n",
    "\n",
    "The reward function is composed of intermediate rewards and terminal rewards: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./raghu_sepsis.png\" height=\"100\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Action Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a given state, taking certain actions may be unwanted or even impossible.  \n",
    "\n",
    "Example:\n",
    "- Prescribing a medication dose which is likely to put a patient at high risk\n",
    "- Moving a robot off a cliff\n",
    "\n",
    "We can prevent the agent from taking such actions with *action masking*\n",
    "\n",
    "This can avoid transitions to bad states and also limit the possible action space\n",
    "\n",
    "How to do this?\n",
    "\n",
    "- For value-based algorithms like Q-Learning, can assign value of $-\\infty$ for these states: $Q(s,a)=-\\infty$\n",
    "- For policy gradient algorithms, set logits to $-\\infty$. This results in assigning probability of zero \n",
    "to unwanted action given the state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### V. Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we review the [Mountain Car Problem](https://gymnasium.farama.org/environments/classic_control/mountain_car/)\n",
    "\n",
    "Moderately difficult problem in RL\n",
    "\n",
    "A car is placed stochastically at the bottom of a sinusoidal valley (see below)\n",
    "\n",
    "State space:\n",
    "- position of the car along the x-axis\n",
    "- velocity of the car\n",
    "\n",
    "Action space: \n",
    "\n",
    "0: Accelerate to the left  \n",
    "1: Donâ€™t accelerate  \n",
    "2: Accelerate to the right\n",
    "\n",
    "\n",
    "Goal: reach the flag at top of mountain at right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mtn_car.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curriculum Learning Strategy**\n",
    "\n",
    "Start with easy environment configurations for the agent and successively increase the difficulty\n",
    "\n",
    "Examples:\n",
    "- For the task of a robot arm grasping an object, the first lesson can situate the arm close to the object \n",
    "- For the mountain car problem , the first lesson can place the car close to the valley dip\n",
    "\n",
    "For each task, iteratively modify the environment with greater challenge until it reaches the full problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caveat:** The \"what got you here won't get you there\" problem.\n",
    "\n",
    "The solution for the easy problem may not work for the harder problem.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
