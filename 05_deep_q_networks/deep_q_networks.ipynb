{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Networks\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: February 10, 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "- Reinforcement Learning, RS Sutton & AG Barto, 2nd edition. Chapter 9\n",
    "- Mastering Reinforcement Learning with Python, Enes Bilgin. Chapter 6\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Explain the novel ideas behind Q-Networks\n",
    "- Explain the new challenges brought with Q-Networks \n",
    "- Explain the purpose of experience replay and implement it in code\n",
    "- Explain the purpose of a target network and implement it in code\n",
    "- Explain the purpose of soft updating the target network\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- function approximation\n",
    "- experience replay\n",
    "- target networks\n",
    "- loss function\n",
    "- soft update of target network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Recap of Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given state space $S$ and action space $A$, learn values $Q(S,A)$.\n",
    "\n",
    "$Q(S,A)$ is the action value function.\n",
    "\n",
    "Values are organized in an array called the *Q-table*.\n",
    "\n",
    "*Q-Learning* is a method for building this table.\n",
    "\n",
    "The optimal action value function is defined as: \n",
    "\n",
    "$Q_*(s,a) := \\underset{\\pi}{\\operatorname{\\max}} Q_{\\pi}(s,a)$\n",
    "\n",
    "This considers the space of policies $\\pi$ for the best policy $\\pi_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Illustration of Q Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![q_table](./Q-Learning_Matrix_Initialized_and_After_Training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. From Q-Learning to Deep Q-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table approach can be limiting: \n",
    "- space of states $S$, actions $A$ can grow large\n",
    "- difficult to explore all combinations $S$ X $A$\n",
    "- difficult to store all values\n",
    "\n",
    "We make a large paradigm shift from computing a finite table of action values $Q(S,A)$ to using function approximation.\n",
    "\n",
    "**Model Type** \n",
    "\n",
    "Many function types have been explored, including: \n",
    "- linear models\n",
    "- polynomials\n",
    "- Fourier basis function\n",
    "- radial basis functions\n",
    "- artificial neural networks\n",
    "\n",
    "Current state of the art uses deep neural networks (DNN) for approximating the action-value function.  \n",
    "This is what we will present going forward.  \n",
    "Denote neural network parameters as $\\theta$ and the action-value function as $Q_{\\theta}$\n",
    "\n",
    "The graphic below illustrates a deep neural network.  \n",
    "The input layer holds each element of the state vector *s*.  \n",
    "The outputs are the action values for each action given the state.  \n",
    "In this case, there are two possible actions from the state, $a_1$ and $a_2$.\n",
    "\n",
    "The outputs are action values: $q(s,a_1)$ and $q(s,a_2)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dqn](./dqn3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this approach brings strengths and weaknesses:\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- can estimate values for unobserved states\n",
    "- can estimate values for massive number of states\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- the function cannot be exactly correct in all states\n",
    "- DNNs require customizations to work in practice; without customizations; the models generally don't converge.\n",
    "- the states will no longer be independent as they share parameters.  \n",
    "This results in a tradeoff where approximations for some states get better while others get worse\n",
    "\n",
    "**Accepting Error and Minimizing it**  \n",
    "\n",
    "For the finite table (Q-learning), convergence to the correct answer was guaranteed.  \n",
    "In the framework of function approximators - where **we can't get correct answers for all states** - we need a metric to optimize.  \n",
    "We want to get \"good enough\" on average, where errors are averaged over a set of points.\n",
    "\n",
    "**Improving the model**  \n",
    "\n",
    "We will use *stochastic gradient descent* to improve the model iteratively, taking an improvement step in the direction of the gradient.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Experience Replay\n",
    "\n",
    "One trick for helping with convergence is *experience replay*.  \n",
    "Experience tuples or transitions $(s_t, a_t, r_t, s_{t+1})$ are stored in a buffer and reused\n",
    "\n",
    "The size of the buffer $M$ is a hyperparameter\n",
    "\n",
    "The tuples can be sampled uniformly at random, which reduces correlation between samples\n",
    "\n",
    "The number of samples, or batch size, is denoted $N$ and it is a hyperparameter.  \n",
    "Note that $N$ can be smaller than $M$\n",
    "\n",
    "In more refined cases, weights can be assigned to each tuple to upweight their probability of selection.  \n",
    "This is called *prioritized experience replay*.  \n",
    "Higher priority can be given to certain $Q(s,a)$ with larger prediction errors.  \n",
    "It can work well to prioritize based on TD errors.\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder on How to Compute TD(0) Updates**\n",
    "\n",
    "We will use TD(0) updating, so let's recall the form:\n",
    "\n",
    "$Q(s,a) := Q(s,a) + \\alpha [r + \\gamma \\underset{a}{\\operatorname{\\max}} Q(s',a) -  Q(s,a)]$\n",
    "\n",
    "The portion\n",
    "\n",
    "$r + \\gamma \\underset{a}{\\operatorname{\\max}} Q(s',a)$\n",
    "\n",
    "comes from the new data.\n",
    "\n",
    "When using deep RL, $Q(s',a)$ will be replaced by $Q_{\\theta'}(s',a)$ which is based on the model.\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### IV. Target Networks\n",
    "\n",
    "Supervised learning pairs features with fixed targets.\n",
    "\n",
    "In deep reinforcement learning, the targets $Q_{\\theta}$ are *q* values which update iteratively.\n",
    "\n",
    "**This is a large complication and without adjustment, it can be hard for algorithm to converge.** \n",
    "\n",
    "Essentially $Q_{\\theta}$ is a moving target.\n",
    "\n",
    "We maintain a second neural network which is a lagged copy of $Q_{\\theta}$ and we call it $Q_{\\theta'}$\n",
    "\n",
    "$Q_{\\theta'}$ is a *lagged neural network*\n",
    "\n",
    "Let's put notation on the target.  \n",
    "For non-terminal state,  \n",
    "\n",
    "$y_j = r_j + \\gamma \\underset{a_j'}{\\operatorname{\\max}} Q_{\\theta'}(s_j',a_j')$\n",
    "\n",
    "For terminal state,  \n",
    "\n",
    "$y_j = r_j$\n",
    "\n",
    "where $y_j$ is the target value at timestep $j$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Updating the Lagged Copy of Network**\n",
    "\n",
    "We use a hyperparameter $C$ to trigger a sync between $Q_{\\theta'}$ and $Q_{\\theta}$.  \n",
    "$C$ can represent the number of time steps until $Q_{\\theta'}$ is updated.\n",
    "\n",
    "When triggered, the update is made: $Q_{\\theta'} := Q_{\\theta}$ \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soft Update of Target Network**\n",
    "\n",
    "The network update method just mentioned is binary: a full update happens at each step or it doesn't.  \n",
    "It is a *hard update*.\n",
    "\n",
    "An alternative is to do a *soft update* like this:\n",
    "\n",
    "- At each time step, slowly update each element of $\\theta'$ with a fraction of each element of $\\theta$ \n",
    "- A smoothing parameter $\\tau$ is used to apply the convex combination\n",
    "- The value of $\\tau$ will be small, such as 0.001\n",
    "\n",
    "$\\theta' = \\tau \\theta + (1-\\tau) \\theta'$ \n",
    "\n",
    "The soft update helps maintain stability in training, while updating the target network.  \n",
    "This practice is used in some DQN implementations and other Deep RL methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function**\n",
    "\n",
    "We need to define and use a loss function as the model won't be exact in each state.  \n",
    "Define the loss as follows:\n",
    "\n",
    "$L(\\theta) = \\mathop{\\mathbb{E}}_{(s,a,r,s')\\sim U(D)} \\left[\\left(r + \\gamma \\underset{a'}{\\operatorname{\\max}} Q_{\\theta'}(s',a') -  Q_{\\theta}(s,a) \\right)^2 \\right]$\n",
    "\n",
    "As a reminder, $D$ is the replay buffer with size $M$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Implementing the Deep Q-Network\n",
    "\n",
    "Now we put the pieces together. Here is the pseudocode, and later we will implement Python code.\n",
    "\n",
    "1. Initialization\n",
    "- Initialize $\\theta$, the neural network parameters\n",
    "- Set target network parameters $\\theta' := \\theta$. This is the neural network with lagged parameters.\n",
    "- Initialize replay buffer $D$ with size $M$\n",
    "- Set minimum batch size $N$ required for model update\n",
    "- Set the hyperparameters for the neural network: number of hidden layers, neurons per hidden layer, ...\n",
    "- Set all remaining hyperparameters: $\\epsilon$, $\\alpha$, $\\gamma$, epochs, steps per epoch, ...\n",
    "\n",
    "2. Set policy $\\pi$ to be $\\epsilon$-greedy with respect to $Q_\\theta$\n",
    "3. Given state $s$ and policy $\\pi$ take action $a$\n",
    "4. Observe reward $r$ and next state $s'$\n",
    "5. Add transition $(s,a,r,s')$ to replay buffer $D$. If $|D|>M$, pop oldest transition.\n",
    "6. If $|D|>N$, uniformly sample random minibatch of size $N$ transitions from $D$, else return to step 2.\n",
    "7. Compute target values $y_j$ for each transition in minibatch.\n",
    "8. Take a gradient step to update $\\theta$, and then update loss function\n",
    "9. Every $C$ time steps, make update $Q_{\\theta'} := Q_{\\theta}$ \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Computational Example of DQN\n",
    "\n",
    "This example updates the model and syncs $Q_{\\theta'} := Q_{\\theta}$ after each transition, so $C=1$.  \n",
    "In practice, we can set $C$ higher.\n",
    "\n",
    "\n",
    "You might want to copy and run this code on [Google Colab](https://colab.research.google.com/?utm_source=scs-index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, LeakyReLU\n",
    "\n",
    "from collections import deque\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_size: 1\n",
      "num_actions: 5\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "epochs = 2\n",
    "time_steps = 30\n",
    "\n",
    "# states\n",
    "sofa_levels = [0,1,2,3]\n",
    "num_states = len(sofa_levels)\n",
    "terminal_state = 3\n",
    "state_size = 1 # dimensions of state space\n",
    "\n",
    "# actions\n",
    "vaso_dose = [0,1,2,3,4]\n",
    "num_actions = len(vaso_dose) # number of possible actions\n",
    "\n",
    "print('state_size:', state_size)\n",
    "print('num_actions:', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on code sourced from: https://github.com/DrAPT/deep-q-learning/blob/master/dqn.py#L38\n",
    "# TF2 + Keras\n",
    "\n",
    "class DQN_Agent():\n",
    "    def __init__(self, state_levels, state_size, action_size, verbose=False):\n",
    "        \n",
    "        self.state_levels  = state_levels\n",
    "        self.state_size    = state_size\n",
    "        self.action_size   = action_size\n",
    "        self.verbose       = verbose\n",
    "        \n",
    "        self.memory_size   = 2000\n",
    "        self.gamma         = 0.95   # discount rate\n",
    "        self.epsilon       = 0.05  # exploration rate\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min   = 0.01\n",
    "        self.leaky_rate    = 0.01\n",
    "        \n",
    "        # model parameters\n",
    "        self.learning_rate = 0.001\n",
    "        self.hidden_1_size = 24\n",
    "        self.hidden_2_size = 24\n",
    "    \n",
    "        self.memory = deque(maxlen=self.memory_size)\n",
    "        \n",
    "        self.model = self._build_dqn_model()\n",
    "\n",
    "    def _build_dqn_model(self):\n",
    "        model = Sequential()\n",
    "        # hidden layer 1\n",
    "        model.add(Dense(self.hidden_1_size, input_dim=self.state_size))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=self.leaky_rate))\n",
    "        # hidden layer 2\n",
    "        model.add(Dense(self.hidden_2_size))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=self.leaky_rate))\n",
    "        # output layer\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=\"adam\")\n",
    "\n",
    "        # initialize model by training on pairs: (state_level, random_qvalues_for_each_action) for each state_level\n",
    "        for st in self.state_levels:\n",
    "            model.fit(np.array([st]).reshape(1,1), np.random.random(self.action_size).reshape(1,self.action_size), verbose=0)\n",
    "            \n",
    "        return model\n",
    " \n",
    "    def act(self, state):\n",
    "        # epsilon-greedy selection of actions\n",
    "        if np.random.rand() <= self.epsilon: # random draw with prob epsilon\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict([state]) # take optimal action\n",
    "        return np.argmax(act_values)  # returns action\n",
    "    \n",
    "    def determine_next_state(self, state, action):\n",
    "        '''\n",
    "        return next state from the environment\n",
    "        NOTE: in practice, this might use a model or historical tuples  \n",
    "        '''\n",
    "        if (state in [0,1,2]) & (action == 0): # no dose raises state\n",
    "            next_state = min(terminal_state, state + 1)\n",
    "        elif action in [3,4]: # higher doses lowers state (floored at zero)\n",
    "            next_state = max(0, state - 1)\n",
    "        else:\n",
    "            next_state = random.choice([1,2])\n",
    "        return next_state\n",
    "        \n",
    "    def compute_reward(self, state):\n",
    "        '''\n",
    "        simple reward function for illustration. lower state value is better.\n",
    "        '''\n",
    "        \n",
    "        if state == 3:\n",
    "            reward = -100\n",
    "        elif state == 2:\n",
    "            reward = -10\n",
    "        elif state == 1:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = 10\n",
    "        return reward\n",
    "\n",
    "        \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        # cache transitions\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        print('==replaying')\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('minibatch size:', len(minibatch))\n",
    "            print(minibatch, '\\n')\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # target is reward if state is termination state\n",
    "            target = reward\n",
    "            if not done:\n",
    "                # value iteration update following Bellman equation\n",
    "                # target equal to reward + predicted discounted future q-value of next state (taking the best action in the state)\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(np.array([next_state]))))\n",
    "            \n",
    "            # target_f = predicted reward + discounted future q-value given the state, for each action (not just the best action)\n",
    "            target_f = self.model.predict(np.array([state]))\n",
    "            \n",
    "            # update q-value for the given action\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            # update the model\n",
    "            self.model.fit(np.array([state]).reshape(1,1), target_f, epochs=1, verbose=0)\n",
    "            \n",
    "            print('==predicted q-fcn, state 0:', self.model.predict(np.array([0])))\n",
    "            print('==predicted q-fcn, state 1:', self.model.predict(np.array([1])))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main**  \n",
    "Simulate trajectories (state, action, reward, next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN_Agent(sofa_levels, state_size, num_actions)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    done = False\n",
    "    state = random.choice([0,1,2]) # exploring starts\n",
    "\n",
    "    for ts in range(time_steps):\n",
    "        action = agent.act(state)\n",
    "        next_state = agent.determine_next_state(state, action) \n",
    "        reward = agent.compute_reward(next_state)\n",
    "        done = True if next_state == terminal_state else False\n",
    "        agent.memorize(state, action, reward, next_state, done)\n",
    "        print('epoch:', ep, ', time_step:', ts, ', state:', state, ', action:', action, ', reward:', reward, ', next_state:', next_state, ', done:', done)\n",
    "        state = next_state\n",
    "        if done: # epoch over\n",
    "            break\n",
    "        \n",
    "        mem_size = len(agent.memory)\n",
    "    \n",
    "        if mem_size > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "            \n",
    "    #print('memory size:', mem_size)\n",
    "    #print('memory:\\n')\n",
    "    #print(agent.memory)\n",
    "            \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "From state 0, which action seems best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Does the Q function seem to converge? Note: DQN doesn't always converge; further methods have been developed to ameliorate this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 - Monte Carlo Simulation**\n",
    "\n",
    "Now that you've trained a Q function, you will use it in a control problem.   \n",
    "Specifically, write code to implement the following:\n",
    "\n",
    "- simulate 10 episodes using 50 time steps each\n",
    "- for each episode, begin in state 0\n",
    "- use the policy to take the next step\n",
    "- get the next state and reward\n",
    "- terminate the episode if the next state = 3\n",
    "- for each time step, print (episode, time_step, state, action, next_state, reward)\n",
    "- compute the cumulative reward for each episode (without discounting)\n",
    "- store and print the cumulative rewards across episodes, computing their min, max, mean\n",
    "- store and print the cumulative medication dose across episodes, computing their min, max, mean\n",
    "\n",
    "Do the results make sense?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 - Multiple Objectives**\n",
    "\n",
    "Suppose that the clinician wishes to meet these simultaneous objectives: \n",
    "\n",
    "- lower SOFA score is better \n",
    "- SOFA of 3 must be avoided\n",
    "- lower total medication dosing is better\n",
    "\n",
    "a) Explain how you could modify the RL problem to achieve these goals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Run an MC simulation following the same procedure as Exercise 3.   \n",
    "Based on the average cumulative reward and average cumulative dose, comment on the results.  \n",
    "How does your approach perform?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
