{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ba6549-8b15-4f95-9615-549dd5fa2f7c",
   "metadata": {},
   "source": [
    "### Lab: Q-Learning to Solve the Cliff Walk Problem\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: December 11, 2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afeab41-bd5f-40e8-ad2f-e8999f13ed45",
   "metadata": {},
   "source": [
    "#### Instructions:\n",
    "\n",
    "In Sutton & Barto (Section 6.5), the Cliff Walking problem is presented. You will apply Q-learning to teach an agent to solve it.  \n",
    "\n",
    "Consider the grid below, where an agent begins in starting state S and wishes to reach goal state G by walking on a path and keeping off The Cliff (the gray region).  At each time step, the agent can potentially take one step left, right, up, or down. There is no discounting.  It is not possible for the agent to move off the grid. If the agent visits The Cliff, it incurs reward -100 and is immediately sent back to the start. The reward is -1 on all transitions apart from The Cliff. This incentivizes the agent to reach G as quickly as possible. \n",
    "\n",
    "**Note**: Do not use libraries from `networkx`, `gym`, `gymnasium` when solving this problem.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266cda1-8a73-4761-a843-676543a67c0d",
   "metadata": {},
   "source": [
    "<img src=\"./cliff_walk.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c322b08-1720-43e0-a3b0-0788fc33b6a7",
   "metadata": {},
   "source": [
    "#### TOTAL POINTS: 12\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a304cd5-72e7-4f53-95d4-1cb9fcfb1d1f",
   "metadata": {},
   "source": [
    "#### 1) Clearly explain how you will set up the state space and action space. \n",
    "**(POINTS: 2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e536f8-9cc1-4295-8283-3d9528ffb8f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86d3e91d-ecb5-4c27-a703-4bd28d307800",
   "metadata": {},
   "source": [
    "#### 2) Write a Q-learning algorithm that helps the agent learn a policy to reach the goal  \n",
    "**(POINTS: 8)**.\n",
    "\n",
    "Use $\\epsilon$-greedy action selection with $\\epsilon=0.1$.  \n",
    "You can decide the other parameters as you wish.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f89e37-d060-48ec-b1a6-809eba649178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "767a4d4a-e3aa-4a24-9e5d-a240fc62cb17",
   "metadata": {},
   "source": [
    "**Note to grader for partial credit:**  \n",
    "Student answers in parts 3-5 should help understand if the algorithm is implemented correctly.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790570e-aa87-40b2-9834-6b1809d22896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51e96184-aaed-4ae4-a60b-805c5b9cd3b9",
   "metadata": {},
   "source": [
    "#### 3) After training the policy, print the row from the Q-table representing the starting state  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118436aa-0c5f-48c1-b591-4db800347b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "393287b5-68ac-4d41-989c-aceab3258b64",
   "metadata": {},
   "source": [
    "#### 4) Based on training results, create a plot that shows final state of each episode on the y-axis, and the episode number on the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af571316-3ec4-437b-ab46-174323417c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2f5f2af-00cc-4010-8481-54b399d794c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5) Given the learned policy, run one episode \n",
    "\n",
    "Show evidence that the agent has learned how to successfully complete the Cliff walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513b617-9f9f-484f-96ca-cf55f3d922a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c67d9ccd-f5f5-4fa4-89b7-160515f13963",
   "metadata": {},
   "source": [
    "#### 6) Explain your results. Do they make sense?\n",
    "**(POINTS: 2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee75f999-5002-4055-88d4-3d2eb67bad50",
   "metadata": {},
   "source": [
    "**Note to grader for partial credit:** Grade based on the reasoning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975fe720-5c2a-4b8f-b5de-ca4a9f161feb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
