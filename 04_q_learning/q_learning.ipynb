{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: February 4, 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "- Reinforcement Learning, RS Sutton & AG Barto, 2nd edition. Chapter 6\n",
    "- Mastering Reinforcement Learning with Python, Enes Bilgin. Chapter 5\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Explain how Q-Learning works and how it learns off policy\n",
    "- Use Q-Learning to compute value functions  \n",
    "- Perform sensitivity analysis on a Q-Learning algorithm\n",
    "- Check for algorithm convergence\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Q-Learning to act off policy\n",
    "- The Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall the big picture of what we're trying to do:  \n",
    "Given state space $S$ and action space $A$, learn values $Q(S,A)$  \n",
    "These are organized in an array called the *Q-table*.\n",
    "\n",
    "Q-Learning is a method for building this table.\n",
    "\n",
    "We initialize the table (zeros, random values with zeros at terminal condition, etc.) and then use TD(0) updates for training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Q-Learning_Matrix_Initialized_and_After_Training.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning is an **off-policy TD control algorithm** that was an early breakthrough in RL.\n",
    "\n",
    "Quick reminder of what off-policy means:\n",
    "\n",
    "We want action-value estimates. To make improvements requires exploring. These two things are at odds.\n",
    "\n",
    "Consider: You're looking for a faster route to work. If you try different routes, some will be slower.  \n",
    "These slower routes shouldn't factor into the timing of the optimal route. You separate optimal route timing from exploration.\n",
    "\n",
    "We do this by maintaining two policies:\n",
    "- behavior policy for learning\n",
    "- target policy for learning optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show the update equation for improving $q_\\pi(s,a)$  \n",
    "It is very similar to the update step for the state value.\n",
    "\n",
    "Since we will use sample data, $Q$ will denote estimates of $q_\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(s,a) := Q(s,a) + \\alpha [r + \\gamma \\underset{a}{\\operatorname{\\max}} Q(s',a) -  Q(s,a)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining the different components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./q_learning_update.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important difference is the $\\underset{a}{\\operatorname{\\max}} Q(s',a)$ term where you might have expected $Q(s',a)$  \n",
    "\n",
    "The agent computes the most valuable action and uses this in updating.\n",
    "\n",
    "However, the agent many not actually take this step when $S_{t+1}=s'$, $A_{t+1}=a$ \n",
    "\n",
    "This is what it means to act off policy: the target policy is separated from the behavior policy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Septic Shock**\n",
    "\n",
    "Next, let's look at a computational example. The objective is to reduce the chance of septic shock, measured by the proxy SOFA score, by using a drug called a vasopressor. The values are for illustration only. Following the code are a series of exercises that we will work through.\n",
    "\n",
    "Background:  \n",
    "- **Septic shock**: a life-threatening condition that happens when blood pressure drops to a dangerously low level after an infection\n",
    "- **Sequential Organ Failure Assessment (SOFA) score** is a scoring system that assesses the performance of several organ systems in the body. We will use this to measure state. Higher is more dangerous.\n",
    "- **Vasopressor (vaso)** a drug that healthcare providers use to make blood vessels constrict (raising blood pressure) in patients with low blood pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1\n",
      "Q \n",
      " [[ 2.881 -0.384  1.420  76.330  14.784]\n",
      " [ 0.187 -0.180  0.779  26.308 -0.704]\n",
      " [-0.694 -0.452 -0.378 -1.366  2.031]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 11\n",
      "Q \n",
      " [[ 227.166  244.070  156.782  604.982  386.889]\n",
      " [ 7.115 -0.180  49.707  518.861  66.899]\n",
      " [-0.694 -0.452 -0.378  366.513  2.031]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 21\n",
      "Q \n",
      " [[ 624.867  557.647  511.056  818.992  682.363]\n",
      " [ 7.115  85.423  174.667  790.114  66.899]\n",
      " [-10.625 -0.452 -0.378  615.842  2.031]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 31\n",
      "Q \n",
      " [[ 819.999  776.733  744.246  922.280  866.842]\n",
      " [ 7.115  213.236  458.334  905.604  144.317]\n",
      " [-10.625  78.523 -0.378  834.631  159.388]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 41\n",
      "Q \n",
      " [[ 923.326  895.659  881.946  965.784  944.518]\n",
      " [ 170.018  213.236  458.334  960.651  144.317]\n",
      " [-19.562  78.523 -0.378  917.481  235.403]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 51\n",
      "Q \n",
      " [[ 959.172  942.558  940.538  985.249  974.593]\n",
      " [ 244.337  213.236  509.586  983.161  303.639]\n",
      " [-19.562  78.523 -0.378  962.181  235.403]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 61\n",
      "Q \n",
      " [[ 975.837  959.515  962.192  993.227  988.446]\n",
      " [ 244.337  354.286  634.066  992.016  434.107]\n",
      " [-27.606  78.523 -0.378  977.935  310.005]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 71\n",
      "Q \n",
      " [[ 984.193  971.619  971.498  997.130  995.588]\n",
      " [ 244.337  354.286  634.066  996.538  490.344]\n",
      " [-27.606  78.523  95.570  983.603  310.005]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 81\n",
      "Q \n",
      " [[ 987.504  977.019  977.703  998.764  997.891]\n",
      " [ 439.892  474.757  669.497  998.566  541.188]\n",
      " [-27.606  78.523  182.648  987.450  377.668]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 91\n",
      "Q \n",
      " [[ 988.858  978.476  973.616  999.484  999.052]\n",
      " [ 492.785  474.757  669.497  999.374  587.001]\n",
      " [-27.606  249.442  182.648  988.834  438.766]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 101\n",
      "Q \n",
      " [[ 989.441  977.348  981.234  999.776  999.650]\n",
      " [ 492.785  474.757  701.505  999.739  698.828]\n",
      " [-27.606  249.442  182.648  989.580  438.766]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 111\n",
      "Q \n",
      " [[ 989.726  976.672  982.435  999.898  999.814]\n",
      " [ 492.785  610.725  701.505  999.875  698.828]\n",
      " [-34.845  249.442  261.367  989.788  438.766]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 121\n",
      "Q \n",
      " [[ 989.906  977.129  982.698  999.955  999.926]\n",
      " [ 540.508  680.973  701.505  999.950  756.040]\n",
      " [-34.845  249.442  261.367  989.922  493.884]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 131\n",
      "Q \n",
      " [[ 989.951  977.137  977.459  999.981  999.958]\n",
      " [ 583.463  711.873  730.351  999.976  780.433]\n",
      " [-34.845  249.442  261.367  989.966  543.492]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 141\n",
      "Q \n",
      " [[ 989.980  979.932  984.225  999.991  999.977]\n",
      " [ 656.919  739.684  754.324  999.989  802.389]\n",
      " [-47.225  323.496  261.367  989.983  543.492]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 151\n",
      "Q \n",
      " [[ 989.989  977.474  980.514  999.996  999.991]\n",
      " [ 656.919  762.724  754.324  999.995  822.149]\n",
      " [-47.225  323.496  261.367  989.993  588.142]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 161\n",
      "Q \n",
      " [[ 989.997  982.546  982.723  999.998  999.996]\n",
      " [ 656.919  783.461  775.901  999.998  855.940]\n",
      " [-47.225  388.156  261.367  989.996  588.142]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 171\n",
      "Q \n",
      " [[ 989.998  978.090  981.083  999.999  999.998]\n",
      " [ 656.919  802.125  812.798  999.999  870.346]\n",
      " [-47.225  388.156  396.026  989.999  588.142]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 181\n",
      "Q \n",
      " [[ 989.999  980.028  978.907  1000.000  999.999]\n",
      " [ 741.790  837.821  828.528  1000.000  883.312]\n",
      " [-47.225  388.156  455.423  989.999  588.142]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 191\n",
      "Q \n",
      " [[ 990.000  976.240  980.056  1000.000  1000.000]\n",
      " [ 764.621  853.039  842.685  1000.000  883.312]\n",
      " [-52.502  388.156  455.423  990.000  588.142]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 201\n",
      "Q \n",
      " [[ 990.000  981.721  981.242  1000.000  1000.000]\n",
      " [ 764.621  853.039  855.427  1000.000  894.980]\n",
      " [-57.252  448.341  455.423  990.000  588.142]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 211\n",
      "Q \n",
      " [[ 990.000  980.044  981.728  1000.000  1000.000]\n",
      " [ 764.621  866.735  879.205  1000.000  914.934]\n",
      " [-61.527  448.341  455.423  990.000  588.142]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 221\n",
      "Q \n",
      " [[ 990.000  982.622  979.789  1000.000  1000.000]\n",
      " [ 835.285  890.155  888.294  1000.000  937.987]\n",
      " [-65.374  448.341  508.881  990.000  628.327]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 231\n",
      "Q \n",
      " [[ 990.000  981.972  982.330  1000.000  1000.000]\n",
      " [ 881.648  900.140  898.465  1000.000  954.792]\n",
      " [-65.374  551.256  555.003  990.000  628.327]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 241\n",
      "Q \n",
      " [[ 990.000  980.786  983.797  1000.000  1000.000]\n",
      " [ 905.619  915.223  919.669  1000.000  959.313]\n",
      " [-68.837  551.256  596.512  990.000  628.327]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 251\n",
      "Q \n",
      " [[ 990.000  975.762  979.795  1000.000  1000.000]\n",
      " [ 923.093  922.701  919.669  1000.000  959.313]\n",
      " [-77.282  551.256  596.512  990.000  628.327]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 261\n",
      "Q \n",
      " [[ 990.000  979.625  980.586  1000.000  1000.000]\n",
      " [ 923.093  927.441  926.702  1000.000  959.313]\n",
      " [-79.554  593.140  596.512  990.000  628.327]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 271\n",
      "Q \n",
      " [[ 990.000  980.129  980.063  1000.000  1000.000]\n",
      " [ 927.794  931.707  946.480  1000.000  963.382]\n",
      " [-79.554  666.753  635.861  990.000  697.045]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 281\n",
      "Q \n",
      " [[ 990.000  977.139  983.730  1000.000  1000.000]\n",
      " [ 932.024  937.536  950.832  1000.000  963.382]\n",
      " [-79.554  666.753  635.861  990.000  697.045]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 291\n",
      "Q \n",
      " [[ 990.000  979.740  975.933  1000.000  1000.000]\n",
      " [ 942.343  937.536  957.458  1000.000  967.044]\n",
      " [-79.554  699.077  671.275  990.000  697.045]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 301\n",
      "Q \n",
      " [[ 990.000  982.237  975.762  1000.000  1000.000]\n",
      " [ 949.865  937.536  957.458  1000.000  975.975]\n",
      " [-79.554  750.572  671.275  990.000  697.045]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 311\n",
      "Q \n",
      " [[ 990.000  980.303  980.414  1000.000  1000.000]\n",
      " [ 951.889  948.351  957.458  1000.000  978.377]\n",
      " [-79.554  750.572  671.275  990.000  697.045]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 321\n",
      "Q \n",
      " [[ 990.000  978.953  979.866  1000.000  1000.000]\n",
      " [ 951.889  948.351  958.723  1000.000  984.237]\n",
      " [-79.554  750.572  671.275  990.000  726.341]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 331\n",
      "Q \n",
      " [[ 990.000  980.358  985.327  1000.000  1000.000]\n",
      " [ 951.889  948.351  965.587  1000.000  985.813]\n",
      " [-79.554  774.514  671.275  990.000  726.341]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 341\n",
      "Q \n",
      " [[ 990.000  981.643  977.549  1000.000  1000.000]\n",
      " [ 956.824  948.351  968.028  1000.000  987.232]\n",
      " [-79.554  774.514  703.148  990.000  726.341]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 351\n",
      "Q \n",
      " [[ 990.000  980.579  979.154  1000.000  1000.000]\n",
      " [ 958.151  950.526  970.225  1000.000  987.232]\n",
      " [-81.598  774.514  703.148  990.000  726.341]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 361\n",
      "Q \n",
      " [[ 990.000  979.540  977.819  1000.000  1000.000]\n",
      " [ 959.346  954.473  970.225  1000.000  988.509]\n",
      " [-81.598  796.063  703.148  990.000  726.341]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 371\n",
      "Q \n",
      " [[ 990.000  977.446  979.108  1000.000  1000.000]\n",
      " [ 962.261  958.026  970.225  1000.000  988.509]\n",
      " [-81.598  796.063  729.843  990.000  752.707]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 381\n",
      "Q \n",
      " [[ 990.000  982.977  979.749  1000.000  1000.000]\n",
      " [ 963.045  958.026  972.192  1000.000  989.658]\n",
      " [-83.439  796.063  755.858  990.000  752.707]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 391\n",
      "Q \n",
      " [[ 990.000  978.320  979.543  1000.000  1000.000]\n",
      " [ 963.750  961.223  972.192  1000.000  990.692]\n",
      " [-85.095  796.063  755.858  990.000  752.707]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 401\n",
      "Q \n",
      " [[ 990.000  983.962  983.195  1000.000  1000.000]\n",
      " [ 964.385  961.223  973.972  1000.000  993.215]\n",
      " [-85.095  796.063  755.858  990.000  752.707]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 411\n",
      "Q \n",
      " [[ 990.000  979.352  982.279  1000.000  1000.000]\n",
      " [ 964.957  964.101  977.018  1000.000  994.504]\n",
      " [-86.585  796.063  755.858  990.000  752.707]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 421\n",
      "Q \n",
      " [[ 990.000  981.597  980.268  1000.000  1000.000]\n",
      " [ 966.350  964.101  977.018  1000.000  995.053]\n",
      " [-86.585  815.457  755.858  990.000  752.707]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 431\n",
      "Q \n",
      " [[ 990.000  978.869  978.538  1000.000  1000.000]\n",
      " [ 966.350  966.691  977.018  1000.000  995.993]\n",
      " [-86.585  815.457  777.283  990.000  752.707]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 441\n",
      "Q \n",
      " [[ 990.000  984.910  980.863  1000.000  1000.000]\n",
      " [ 966.350  969.227  977.018  1000.000  996.755]\n",
      " [-86.585  832.911  777.283  990.000  776.436]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 451\n",
      "Q \n",
      " [[ 990.000  980.102  980.966  1000.000  1000.000]\n",
      " [ 966.350  969.227  977.494  1000.000  997.079]\n",
      " [-87.927  832.911  777.283  990.000  776.436]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 461\n",
      "Q \n",
      " [[ 990.000  980.793  981.120  1000.000  1000.000]\n",
      " [ 966.350  969.463  976.089  1000.000  997.371]\n",
      " [-87.927  848.620  777.283  990.000  776.436]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 471\n",
      "Q \n",
      " [[ 990.000  983.761  981.852  1000.000  1000.000]\n",
      " [ 966.350  971.517  975.490  1000.000  997.371]\n",
      " [-89.134  848.620  777.283  990.000  776.436]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 481\n",
      "Q \n",
      " [[ 990.000  981.555  978.259  1000.000  1000.000]\n",
      " [ 966.725  973.365  975.490  1000.000  997.634]\n",
      " [-89.134  860.768  798.554  990.000  817.013]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 491\n",
      "Q \n",
      " [[ 990.000  977.839  980.620  1000.000  1000.000]\n",
      " [ 966.725  973.039  975.490  1000.000  997.871]\n",
      " [-89.134  860.768  833.138  990.000  834.312]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "\n",
    "# Initialize states, actions, Q function\n",
    "\n",
    "# states\n",
    "sofa_levels = [0,1,2,3]\n",
    "num_states = len(sofa_levels)\n",
    "terminal_state = 3\n",
    "\n",
    "# actions\n",
    "vaso_dose = [0,1,2,3,4]\n",
    "num_actions = len(vaso_dose)\n",
    "\n",
    "# initialize array to store action values Q\n",
    "Q = np.random.normal(size=(num_states, num_actions))\n",
    "Q[terminal_state,:] = 0 # no action taken from terminal state, so no value\n",
    "\n",
    "\n",
    "def act(epsilon, action_values):\n",
    "    '''\n",
    "    epsilon-greedy policy: return action using epsilon-greedy strategy\n",
    "    '''\n",
    "    action_size = len(action_values)\n",
    "    if np.random.rand() <= epsilon: # random draw with prob epsilon\n",
    "        return random.randrange(action_size)\n",
    "    return np.argmax(action_values)  # returns action\n",
    "\n",
    "def calc_reward(state):\n",
    "    '''\n",
    "    simple reward function for illustration. lower state value is better\n",
    "    '''\n",
    "    if state == 3:\n",
    "        reward = -100\n",
    "    elif state == 2:\n",
    "        reward = -10\n",
    "    elif state == 1:\n",
    "        reward = 0\n",
    "    else:\n",
    "        reward = 10\n",
    "    return reward\n",
    "\n",
    "def determine_next_state(state, action):\n",
    "    '''\n",
    "    return next state from the environment\n",
    "    to be replaced with simulated data or alternative    \n",
    "    '''\n",
    "    if (state in [0,1,2]) & (action == 0): # no dose raises state\n",
    "        next_state = min(terminal_state, state + 1)\n",
    "    elif action in [3,4]: # higher doses lowers state (floored at zero)\n",
    "        next_state = max(0, state - 1)\n",
    "    else:\n",
    "        next_state = random.choice([1,2])\n",
    "    return next_state\n",
    "\n",
    "# Run the Process\n",
    "num_episodes = 500\n",
    "max_timesteps = 100\n",
    "epsilon = 0.1\n",
    "alpha = 0.1 # weight on new data \n",
    "gamma = 0.99 # discount factor\n",
    "verbose = False\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    if ep % 10 == 0:\n",
    "        print('episode',ep+1)\n",
    "    #print('(state,action,reward,next_state) transitions')\n",
    "    sofa_level = 0 # initialize state\n",
    "    done = False\n",
    "    for tm in range(max_timesteps):\n",
    "        \n",
    "        # given state, get action from policy\n",
    "        vaso_dose = act(epsilon, Q[sofa_level,:])\n",
    "        \n",
    "        next_sofa = determine_next_state(sofa_level, vaso_dose)\n",
    "        reward = calc_reward(next_sofa)\n",
    "        transition = (sofa_level,vaso_dose,reward,next_sofa, done)\n",
    "        \n",
    "        if verbose:\n",
    "            print(transition)\n",
    "        \n",
    "        # update Q(S,A) using TD(0)\n",
    "        # Q(S,A) = Q(S,A) + alpha (r + gamma * max_a Q(S',a) - Q(S,A))\n",
    "        Q[sofa_level,vaso_dose] += alpha*(reward+gamma*np.amax(Q[next_sofa,:])-Q[sofa_level,vaso_dose])        \n",
    "                \n",
    "        sofa_level = next_sofa # update sofa for next iteration\n",
    "        \n",
    "        # terminal state check\n",
    "        if next_sofa == terminal_state:\n",
    "            done = True\n",
    "            break\n",
    "    if ep % 10 == 0:\n",
    "        print('Q \\n', Q)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "If the agent is in state 0, what is the most valuable action? what is least valuable action? Enter your final Q estimate here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "How do your answers change with different $\\alpha$? different $\\epsilon$? Enter your final Q estimates here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "\n",
    "We initialized Q with standard normal deviates. How do your answers in (1) change if you initialize Q with zeros?  \n",
    "Enter your final Q estimates here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**\n",
    "\n",
    "Does Q seem to converge? It will converge given enough iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**\n",
    "\n",
    "Modify the code to return all transitions as a list of tuples. Paste the first 10 transitions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Limitations of Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've learned, Q-learning involves storing and updating a table or array of values $Q(S,A)$ where each element represents the value of a *(state,action)* tuple. This is called a *Q table*.\n",
    "\n",
    "**As the number of states and actions (the *state-action space*) grows, this approach becomes unmanageable** in terms of both storage and computation. This occurs for continuous variables or discrete variables with a massive number of possible values.\n",
    "\n",
    "There are two approaches to handle this issue:\n",
    "\n",
    "- Quantize the values \n",
    "\n",
    "For example, medication doses might be bucketed into dose ranges  \n",
    "\n",
    "- Function approximators for Q  \n",
    "\n",
    "The function approximation is now very popular, with neural nets playing a major role.\n",
    "\n",
    "**Going Deep**\n",
    "\n",
    "When deep neural networks are used with Q-Learning, the model is called a *Deep Q-Network*. We will study these next.\n",
    "\n",
    "In general, pairing reinforcement learning with a deep neural network is called *Deep Reinforcement Learning*, abbreviated Deep RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
