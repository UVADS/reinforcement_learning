{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: February 4, 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "- Reinforcement Learning, RS Sutton & AG Barto, 2nd edition. Chapter 6\n",
    "- Mastering Reinforcement Learning with Python, Enes Bilgin. Chapter 5\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Explain how Q-Learning works and how it learns off policy\n",
    "- Use Q-Learning to compute value functions  \n",
    "- Perform sensitivity analysis on a Q-Learning algorithm\n",
    "- Check for algorithm convergence\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Q-Learning to act off policy\n",
    "- The Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall the big picture of what we're trying to do:  \n",
    "Given state space $S$ and action space $A$, learn values $Q(S,A)$  \n",
    "These are organized in an array called the *Q-table*.\n",
    "\n",
    "Q-Learning is a method for building this table.\n",
    "\n",
    "We initialize the table (zeros, random values with zeros at terminal condition, etc.) and then use TD(0) updates for training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Q-Learning_Matrix_Initialized_and_After_Training.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning is an **off-policy TD control algorithm** that was an early breakthrough in RL.\n",
    "\n",
    "Quick reminder of what off-policy means:\n",
    "\n",
    "We want action-value estimates. To make improvements requires exploring. These two things are at odds.\n",
    "\n",
    "Consider: You're looking for a faster route to work. If you try different routes, some will be slower.  \n",
    "These slower routes shouldn't factor into the timing of the optimal route. You separate optimal route timing from exploration.\n",
    "\n",
    "We do this by maintaining two policies:\n",
    "- behavior policy for learning\n",
    "- target policy for learning optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show the update equation for improving $q_\\pi(s,a)$  \n",
    "It is very similar to the update step for the state value.\n",
    "\n",
    "Since we will use sample data, $Q$ will denote estimates of $q_\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(s,a) := Q(s,a) + \\alpha [r + \\gamma \\underset{a}{\\operatorname{\\max}} Q(s',a) -  Q(s,a)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining the different components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./q_learning_update.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important difference is the $\\underset{a}{\\operatorname{\\max}} Q(s',a)$ term where you might have expected $Q(s',a)$  \n",
    "\n",
    "The agent computes the most valuable action and uses this in updating.\n",
    "\n",
    "However, the agent many not actually take this step when $S_{t+1}=s'$, $A_{t+1}=a$ \n",
    "\n",
    "This is what it means to act off policy: the target policy is separated from the behavior policy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Septic Shock**\n",
    "\n",
    "Next, let's look at a computational example. The objective is to reduce the chance of septic shock, measured by the proxy SOFA score, by using a drug called a vasopressor. The values are for illustration only. Following the code are a series of exercises that we will work through.\n",
    "\n",
    "Background:  \n",
    "- **Septic shock**: a life-threatening condition that happens when blood pressure drops to a dangerously low level after an infection\n",
    "- **Sequential Organ Failure Assessment (SOFA) score** is a scoring system that assesses the performance of several organ systems in the body. We will use this to measure state. Higher is more dangerous.\n",
    "- **Vasopressor (vaso)** a drug that healthcare providers use to make blood vessels constrict (raising blood pressure) in patients with low blood pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1\n",
      "Q \n",
      " [[ 19.979 -1.285  1.802  1.626  2.076]\n",
      " [-1.258 -1.489  1.434  25.129  0.406]\n",
      " [ 1.943  0.950  1.550 -2.026  6.021]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 11\n",
      "Q \n",
      " [[ 199.096  84.536  90.657  43.348  78.608]\n",
      " [ 35.948  68.708  65.534  204.356  83.193]\n",
      " [ 1.943  0.950  1.550 -2.026  149.253]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 21\n",
      "Q \n",
      " [[ 307.531  195.902  218.417  189.980  197.118]\n",
      " [ 187.201  214.764  181.683  312.754  201.592]\n",
      " [ 1.943  21.865  57.709  23.502  287.378]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 31\n",
      "Q \n",
      " [[ 378.836  269.203  289.773  321.191  296.756]\n",
      " [ 219.162  289.782  242.016  383.690  289.141]\n",
      " [-8.252  21.865  57.709  23.502  350.013]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 41\n",
      "Q \n",
      " [[ 419.054  331.583  370.582  377.078  349.099]\n",
      " [ 339.976  355.255  341.697  424.039  361.412]\n",
      " [-17.426  21.865  88.489  126.024  408.803]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 51\n",
      "Q \n",
      " [[ 448.284  396.747  407.528  429.184  406.063]\n",
      " [ 391.808  405.516  396.519  453.373  416.720]\n",
      " [-17.426  21.865  120.664  157.160  441.324]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 61\n",
      "Q \n",
      " [[ 465.752  427.330  427.192  450.416  432.503]\n",
      " [ 412.373  435.164  423.755  470.758  455.468]\n",
      " [-25.684  21.865  152.187  214.204  458.860]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 71\n",
      "Q \n",
      " [[ 477.701  444.467  451.235  471.272  469.547]\n",
      " [ 433.108  448.612  447.779  482.706  467.323]\n",
      " [-25.684  66.430  152.187  214.204  473.342]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 81\n",
      "Q \n",
      " [[ 485.008  453.353  470.946  478.468  481.759]\n",
      " [ 448.198  463.280  469.982  490.027  479.448]\n",
      " [-33.115  66.430  152.187  214.204  483.182]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 91\n",
      "Q \n",
      " [[ 489.350  465.592  475.325  484.803  487.307]\n",
      " [ 467.029  474.339  477.510  494.381  486.654]\n",
      " [-45.824  108.418  152.187  241.506  488.088]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 101\n",
      "Q \n",
      " [[ 492.744  482.237  479.170  490.888  546.575]\n",
      " [ 472.673  479.270  479.693  508.286  492.961]\n",
      " [-51.241  144.963  184.417  266.345  491.760]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 111\n",
      "Q \n",
      " [[ 695.726  598.672  602.219  691.436  805.699]\n",
      " [ 495.571  499.770  479.693  786.238  515.187]\n",
      " [-51.241  144.963  184.417  266.345  658.029]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 121\n",
      "Q \n",
      " [[ 842.943  720.059  780.063  860.201  917.485]\n",
      " [ 537.438  534.911  542.873  901.532  580.631]\n",
      " [-51.241  208.988  184.417  266.345  822.801]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 131\n",
      "Q \n",
      " [[ 912.872  858.218  887.934  931.487  965.307]\n",
      " [ 570.967  663.614  542.873  960.072  618.014]\n",
      " [-51.241  208.988  184.417  266.345  915.110]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 141\n",
      "Q \n",
      " [[ 958.375  934.821  940.966  973.673  984.999]\n",
      " [ 606.319  663.614  611.499  983.149  686.122]\n",
      " [-51.241  283.460  258.979  336.567  960.898]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 151\n",
      "Q \n",
      " [[ 977.419  958.040  960.132  987.880  993.540]\n",
      " [ 672.167  663.614  648.210  992.680  716.536]\n",
      " [-51.241  283.460  258.979  336.567  978.872]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 161\n",
      "Q \n",
      " [[ 985.224  971.295  970.909  995.156  997.209]\n",
      " [ 727.298  774.045  682.060  996.753  716.536]\n",
      " [-51.241  283.460  258.979  336.567  984.764]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 171\n",
      "Q \n",
      " [[ 987.722  978.674  976.699  997.439  998.793]\n",
      " [ 825.362  810.750  712.555  998.583  744.755]\n",
      " [-51.241  353.922  258.979  336.567  987.826]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 181\n",
      "Q \n",
      " [[ 988.962  976.800  982.782  999.202  999.487]\n",
      " [ 839.629  826.539  712.555  999.406  744.755]\n",
      " [-51.241  417.465  332.000  336.567  988.810]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 191\n",
      "Q \n",
      " [[ 989.507  979.871  979.489  999.637  999.777]\n",
      " [ 852.631  869.120  712.555  999.747  744.755]\n",
      " [-51.241  524.135  332.000  336.567  989.598]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 201\n",
      "Q \n",
      " [[ 989.800  981.560  981.653  999.860  999.897]\n",
      " [ 852.631  879.188  787.700  999.881  744.755]\n",
      " [-60.505  524.135  332.000  336.567  989.732]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 211\n",
      "Q \n",
      " [[ 989.910  977.788  982.825  999.931  999.952]\n",
      " [ 864.361  890.257  805.920  999.946  770.272]\n",
      " [-64.455  524.135  332.000  336.567  989.900]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 221\n",
      "Q \n",
      " [[ 989.951  984.913  976.741  999.962  999.978]\n",
      " [ 874.930  890.257  824.324  999.973  813.914]\n",
      " [-68.009  568.726  397.795  336.567  989.953]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 231\n",
      "Q \n",
      " [[ 989.976  980.989  978.926  999.985  999.991]\n",
      " [ 900.713  890.257  824.324  999.989  849.269]\n",
      " [-68.009  568.726  457.014  401.909  989.982]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 241\n",
      "Q \n",
      " [[ 989.992  978.337  977.605  999.992  999.996]\n",
      " [ 907.651  890.257  824.324  999.995  877.907]\n",
      " [-68.009  648.766  508.321  401.909  989.992]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 251\n",
      "Q \n",
      " [[ 989.997  977.010  979.544  999.997  999.998]\n",
      " [ 907.651  890.257  840.891  999.998  877.907]\n",
      " [-68.009  648.766  508.321  460.718  989.997]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 261\n",
      "Q \n",
      " [[ 989.998  979.831  976.324  999.998  999.999]\n",
      " [ 913.896  898.241  855.802  999.999  901.104]\n",
      " [-68.009  648.766  508.321  460.718  989.999]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 271\n",
      "Q \n",
      " [[ 989.999  981.406  981.933  999.999  1000.000]\n",
      " [ 924.574  905.427  869.222  1000.000  910.994]\n",
      " [-71.208  648.766  554.498  460.718  989.999]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 281\n",
      "Q \n",
      " [[ 990.000  983.871  982.311  1000.000  1000.000]\n",
      " [ 929.127  921.496  869.222  1000.000  919.894]\n",
      " [-71.208  680.900  554.498  460.718  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 291\n",
      "Q \n",
      " [[ 990.000  982.220  984.305  1000.000  1000.000]\n",
      " [ 929.127  926.356  901.953  1000.000  919.894]\n",
      " [-71.208  680.900  633.463  460.718  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 301\n",
      "Q \n",
      " [[ 990.000  984.453  981.758  1000.000  1000.000]\n",
      " [ 936.912  930.731  924.022  1000.000  919.894]\n",
      " [-74.088  680.900  633.463  561.281  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 311\n",
      "Q \n",
      " [[ 990.000  978.990  981.879  1000.000  1000.000]\n",
      " [ 943.217  930.731  936.558  1000.000  919.894]\n",
      " [-76.679  680.900  633.463  561.281  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 321\n",
      "Q \n",
      " [[ 990.000  982.124  978.342  1000.000  1000.000]\n",
      " [ 943.217  941.400  939.912  1000.000  941.603]\n",
      " [-76.679  680.900  633.463  604.153  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 331\n",
      "Q \n",
      " [[ 990.000  983.906  979.244  1000.000  1000.000]\n",
      " [ 945.906  944.270  947.638  1000.000  947.443]\n",
      " [-76.679  711.810  633.463  604.153  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 341\n",
      "Q \n",
      " [[ 990.000  979.477  978.479  1000.000  1000.000]\n",
      " [ 954.226  954.872  959.118  1000.000  952.698]\n",
      " [-76.679  739.629  667.126  708.718  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 351\n",
      "Q \n",
      " [[ 990.000  977.093  979.588  1000.000  1000.000]\n",
      " [ 957.242  956.395  963.195  1000.000  952.698]\n",
      " [-76.679  764.666  667.126  736.846  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 361\n",
      "Q \n",
      " [[ 990.000  982.664  978.038  1000.000  1000.000]\n",
      " [ 960.727  956.395  963.885  1000.000  957.429]\n",
      " [-79.011  764.666  726.482  736.846  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 371\n",
      "Q \n",
      " [[ 990.000  978.234  978.673  1000.000  1000.000]\n",
      " [ 963.267  959.755  972.866  1000.000  961.686]\n",
      " [-81.110  785.209  750.844  736.846  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 381\n",
      "Q \n",
      " [[ 990.000  980.079  979.234  1000.000  1000.000]\n",
      " [ 963.267  963.711  972.866  1000.000  965.517]\n",
      " [-82.999  785.209  750.844  736.846  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 391\n",
      "Q \n",
      " [[ 990.000  979.818  980.867  1000.000  1000.000]\n",
      " [ 964.565  966.340  972.589  1000.000  968.965]\n",
      " [-82.999  803.698  750.844  736.846  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 401\n",
      "Q \n",
      " [[ 990.000  981.496  982.266  1000.000  1000.000]\n",
      " [ 965.119  966.340  974.331  1000.000  968.965]\n",
      " [-82.999  803.698  750.844  762.161  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 411\n",
      "Q \n",
      " [[ 990.000  976.999  979.638  1000.000  1000.000]\n",
      " [ 965.119  971.140  975.517  1000.000  972.069]\n",
      " [-82.999  803.698  750.844  762.161  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 421\n",
      "Q \n",
      " [[ 990.000  983.245  979.387  1000.000  1000.000]\n",
      " [ 966.469  971.036  974.975  1000.000  972.069]\n",
      " [-86.229  803.698  750.844  805.451  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 431\n",
      "Q \n",
      " [[ 990.000  979.669  978.025  1000.000  1000.000]\n",
      " [ 966.469  970.942  974.488  1000.000  974.862]\n",
      " [-86.229  803.698  750.844  823.906  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 441\n",
      "Q \n",
      " [[ 990.000  981.139  981.673  1000.000  1000.000]\n",
      " [ 966.832  970.942  974.049  1000.000  977.376]\n",
      " [-86.229  803.698  774.760  823.906  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 451\n",
      "Q \n",
      " [[ 990.000  974.909  976.730  1000.000  1000.000]\n",
      " [ 967.717  970.942  973.654  1000.000  979.638]\n",
      " [-87.606  850.584  794.294  823.906  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 461\n",
      "Q \n",
      " [[ 990.000  978.602  981.287  1000.000  1000.000]\n",
      " [ 967.956  972.848  973.654  1000.000  983.507]\n",
      " [-87.606  850.584  811.874  840.515  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 471\n",
      "Q \n",
      " [[ 990.000  979.714  978.071  1000.000  1000.000]\n",
      " [ 968.170  972.573  973.654  1000.000  985.156]\n",
      " [-87.606  850.584  811.874  840.515  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 481\n",
      "Q \n",
      " [[ 990.000  978.938  978.609  1000.000  1000.000]\n",
      " [ 968.363  972.573  977.285  1000.000  985.156]\n",
      " [-88.846  864.526  845.718  840.515  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n",
      "episode 491\n",
      "Q \n",
      " [[ 990.000  980.125  980.270  1000.000  1000.000]\n",
      " [ 968.363  972.573  977.285  1000.000  986.641]\n",
      " [-88.846  864.526  860.146  855.464  990.000]\n",
      " [ 0.000  0.000  0.000  0.000  0.000]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "\n",
    "# Initialize states, actions, Q function\n",
    "\n",
    "# states\n",
    "sofa_levels = [0,1,2,3]\n",
    "num_states = len(sofa_levels)\n",
    "terminal_state = 3\n",
    "\n",
    "# actions\n",
    "vaso_dose = [0,1,2,3,4]\n",
    "num_actions = len(vaso_dose)\n",
    "\n",
    "# initialize array to store action values Q\n",
    "Q = np.random.normal(size=(num_states, num_actions))\n",
    "Q[terminal_state,:] = 0 # no action taken from terminal state, so no value\n",
    "\n",
    "\n",
    "def act(epsilon, action_values):\n",
    "    '''\n",
    "    epsilon-greedy policy: return action using epsilon-greedy strategy\n",
    "    '''\n",
    "    action_size = len(action_values)\n",
    "    if np.random.rand() <= epsilon: # random draw with prob epsilon\n",
    "        return random.randrange(action_size)\n",
    "    return np.argmax(action_values)  # returns action\n",
    "\n",
    "def calc_reward(state):\n",
    "    '''\n",
    "    simple reward function for illustration. lower state value is better\n",
    "    '''\n",
    "    if state == 3:\n",
    "        reward = -100\n",
    "    elif state == 2:\n",
    "        reward = -10\n",
    "    elif state == 1:\n",
    "        reward = 0\n",
    "    else:\n",
    "        reward = 10\n",
    "    return reward\n",
    "\n",
    "def determine_next_state(state, action):\n",
    "    '''\n",
    "    return next state from the environment\n",
    "    to be replaced with simulated data or alternative    \n",
    "    '''\n",
    "    if (state in [0,1,2]) & (action == 0): # no dose raises state\n",
    "        next_state = min(terminal_state, state + 1)\n",
    "    elif action in [3,4]: # higher doses lowers state (floored at zero)\n",
    "        next_state = max(0, state - 1)\n",
    "    else:\n",
    "        next_state = random.choice([1,2])\n",
    "    return next_state\n",
    "\n",
    "# Run the Process\n",
    "num_episodes = 500\n",
    "max_timesteps = 100\n",
    "epsilon = 0.1\n",
    "alpha = 0.1 # weight on new data \n",
    "gamma = 0.99 # discount factor\n",
    "verbose = False\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    if ep % 10 == 0:\n",
    "        print('episode',ep+1)\n",
    "    #print('(state,action,reward,next_state) transitions')\n",
    "    sofa_level = 0 # initialize state\n",
    "    done = False\n",
    "    for tm in range(max_timesteps):\n",
    "        \n",
    "        # given state, get action from policy\n",
    "        vaso_dose = act(epsilon, Q[sofa_level,:])\n",
    "        \n",
    "        next_sofa = determine_next_state(sofa_level, vaso_dose)\n",
    "        reward = calc_reward(next_sofa)\n",
    "        transition = (sofa_level,vaso_dose,reward,next_sofa, done)\n",
    "        \n",
    "        if verbose:\n",
    "            print(transition)\n",
    "        \n",
    "        # update Q(S,A) using TD(0)\n",
    "        # Q(S,A) = Q(S,A) + alpha (r + gamma * max_a Q(S',a) - Q(S,A))\n",
    "        Q[sofa_level,vaso_dose] += alpha*(reward+gamma*np.amax(Q[next_sofa,:])-Q[sofa_level,vaso_dose])        \n",
    "                \n",
    "        sofa_level = next_sofa # update sofa for next iteration\n",
    "        \n",
    "        # terminal state check\n",
    "        if next_sofa == terminal_state:\n",
    "            done = True\n",
    "            break\n",
    "    if ep % 10 == 0:\n",
    "        print('Q \\n', Q)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "If the agent is in state 0, what is the most valuable action? what is least valuable action? Enter your final Q estimate here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "How do your answers change with different $\\alpha$? different $\\epsilon$? Enter your final Q estimates here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "\n",
    "We initialized Q with standard normal deviates. How do your answers in (1) change if you initialize Q with zeros?  \n",
    "Enter your final Q estimates here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**\n",
    "\n",
    "Does Q seem to converge? It will converge given enough iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**\n",
    "\n",
    "Modify the code to return all transitions as a list of tuples. Paste the first 10 transitions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Limitations of Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've learned, Q-learning involves storing and updating a table or array of values $Q(S,A)$ where each element represents the value of a *(state,action)* tuple. This is called a *Q table*.\n",
    "\n",
    "**As the number of states and actions (the *state-action space*) grows, this approach becomes unmanageable** in terms of both storage and computation. This occurs for continuous variables or discrete variables with a massive number of possible values.\n",
    "\n",
    "There are two approaches to handle this issue:\n",
    "\n",
    "- Quantize the values \n",
    "\n",
    "For example, medication doses might be bucketed into dose ranges  \n",
    "\n",
    "- Function approximators for Q  \n",
    "\n",
    "The function approximation is now very popular, with neural nets playing a major role.\n",
    "\n",
    "**Going Deep**\n",
    "\n",
    "When deep neural networks are used with Q-Learning, the model is called a *Deep Q-Network*. We will study these next.\n",
    "\n",
    "In general, pairing reinforcement learning with a deep neural network is called *Deep Reinforcement Learning*, abbreviated Deep RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
